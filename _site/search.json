[
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_3.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_3.html",
    "title": "Hands-on_Ex04_3",
    "section": "",
    "text": "11 Funnel Plots for Fair Comparisons\n11.1 Overview\nFunnel plot is a specially designed data visualisation for conducting unbiased comparison between outlets, stores or business entities. By the end of this hands-on exercise, you will gain hands-on experience on:"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_3.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_3.html#installing-and-launching-r-packages",
    "title": "Hands-on_Ex04_3",
    "section": "11.2 Installing and Launching R Packages",
    "text": "11.2 Installing and Launching R Packages\nIn this exercise, four R packages will be used. They are:\n\nreadr for importing csv into R.\nFunnelPlotR for creating funnel plot.\nggplot2 for creating funnel plot manually.\nknitr for building static html table.\nplotly for creating interactive funnel plot.\n\n\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)\n\n11.3 Importing Data\nIn this section, COVID-19_DKI_Jakarta will be used. The data was downloaded from Open Data Covid-19 Provinsi DKI Jakarta portal. For this hands-on exercise, we are going to compare the cumulative COVID-19 cases and death by sub-district (i.e. kelurahan) as at 31st July 2021, DKI Jakarta.\nThe code chunk below imports the data into R and save it into a tibble data frame object called covid19.\n\ncovid19 <- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %>%\n  mutate_if(is.character, as.factor)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_3.html#funnelplotr-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_3.html#funnelplotr-methods",
    "title": "Hands-on_Ex04_3",
    "section": "11.4 FunnelPlotR methods",
    "text": "11.4 FunnelPlotR methods\nFunnelPlotR package uses ggplot to generate funnel plots. It requires a numerator (events of interest), denominator (population to be considered) and group. The key arguments selected for customisation are:\n\nlimit: plot limits (95 or 99).\nlabel_outliers: to label outliers (true or false).\nPoisson_limits: to add Poisson limits to the plot.\nOD_adjust: to add overdispersed limits to the plot.\nxrange and yrange: to specify the range to display for axes, acts like a zoom function.\nOther aesthetic components such as graph title, axis labels etc.\n\n\n11.4.1 FunnelPlotR methods: The basic plot\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  numerator = covid19$Positive,\n  denominator = covid19$Death,\n  group = covid19$`Sub-district`\n)\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \nThings to learn from the code chunk above.\n\ngroup in this function is different from the scatterplot. Here, it defines the level of the points to be plotted i.e. Sub-district, District or City. If Cityc is chosen, there are only six data points.\nBy default, data_typeargument is “SR”.\nlimit: Plot limits, accepted values are: 95 or 99, corresponding to 95% or 99.8% quantiles of the distribution.\n\n\n\n11.4.2 FunnelPlotR methods: Makeover 1\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",     #<<\n  x_range = c(0, 6500),  #<<\n  y_range = c(0, 0.05)   #<<\n)\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nA funnel plot object with 267 points of which 7 are outliers. Plot is adjusted for overdispersion.\nThings to learn from the code chunk above. + data_type argument is used to change from default “SR” to “PR” (i.e. proportions). + xrange and yrange are used to set the range of x-axis and y-axis\n\n\n11.4.3 FunnelPlotR methods: Makeover 2\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",   \n  x_range = c(0, 6500),  \n  y_range = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate \\nby Cumulative Total Number of COVID-19 Positive Cases\", #<<           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #<<\n  y_label = \"Cumulative Fatality Rate\"  #<<\n) \n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \nThings to learn from the code chunk above.\n\nlabel = NA argument is to removed the default label outliers feature.\ntitle argument is used to add plot title.\nx_label and y_label arguments are used to add/edit x-axis and y-axis titles."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_3.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_3.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "title": "Hands-on_Ex04_3",
    "section": "11.5 Funnel Plot for Fair Visual Comparison: ggplot2 methods",
    "text": "11.5 Funnel Plot for Fair Visual Comparison: ggplot2 methods\nIn this section, you will gain hands-on experience on building funnel plots step-by-step by using ggplot2. It aims to enhance you working experience of ggplot2 to customise speciallised data visualisation like funnel plot.\n\n11.5.1 Computing the basic derived fields\nTo plot the funnel plot from scratch, we need to derive cumulative death rate and standard error of cumulative death rate.\n\ndf <- covid19 %>%\n  mutate(rate = Death / Positive) %>%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %>%\n  filter(rate > 0)\n\nNext, the fit.mean is computed by using the code chunk below.\n\nfit.mean <- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\n11.5.2 Calculate lower and upper limits for 95% and 99.9% CI\nThe code chunk below is used to compute the lower and upper limits for 95% confidence interval.\n\nnumber.seq <- seq(1, max(df$Positive), 1)\nnumber.ll95 <- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 <- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 <- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 <- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI <- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\n11.5.3 Plotting a static funnel plot\nIn the code chunk below, ggplot2 functions are used to plot a static funnel plot.\n\np <- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\n\nWarning in geom_point(aes(label = `Sub-district`), alpha = 0.4): Ignoring\nunknown aesthetics: label\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\np\n\n\n\n\n\n\n11.5.4 Interactive Funnel Plot: plotly + ggplot2\nThe funnel plot created using ggplot2 functions can be made interactive with ggplotly() of plotly r package.\n\nfp_ggplotly <- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_3.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_3.html#references",
    "title": "Hands-on_Ex04_3",
    "section": "11.6 References",
    "text": "11.6 References\n\nfunnelPlotR package.\nFunnel Plots for Indirectly-standardised ratios.\nChanging funnel plot options\nggplot2 package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_2.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_2.html",
    "title": "Hands-on_Ex04.2",
    "section": "",
    "text": "Visualising the uncertainty of point estimates\n\n\n\n\n\n\n\nNote\n\n\n\n\nA point estimate is a single number, such as a mean.\nUncertainty is expressed as standard error, confidence interval, or credible interval\nImportant:\n\nDon’t confuse the uncertainty of a point estimate with the variation in the sample\n\n\n\n\n\npacman::p_load(tidyverse, plotly, crosstalk, DT, ggdist, gganimate)\nexam <- read_csv(\"data/Exam_data.csv\")\n\n\n\n\ngroup the observation by RACE,\ncomputes the count of observations, mean, standard deviation and standard error of Maths by RACE, and\nsave the output as a tibble data table called my_sum.\n\n\nmy_sum <- exam %>%\n  group_by(RACE) %>%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %>%\n  mutate(se=sd/sqrt(n-1))\n\nNext, the code chunk below will\n\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n \n  \n    RACE \n    n \n    mean \n    sd \n    se \n  \n \n\n  \n    Chinese \n    193 \n    76.50777 \n    15.69040 \n    1.132357 \n  \n  \n    Indian \n    12 \n    60.66667 \n    23.35237 \n    7.041005 \n  \n  \n    Malay \n    108 \n    57.44444 \n    21.13478 \n    2.043177 \n  \n  \n    Others \n    9 \n    69.66667 \n    10.72381 \n    3.791438 \n  \n\n\n\n\n\n\n\n\nThe code chunk below is used to reveal the standard error of mean maths score by race.\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean \n          maths score by race\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x= reorder(RACE,mean), \n        ymin=mean - 1.96*se, \n        ymax=mean + 1.96*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"95% confidence interval of mean maths score by race\")\n\n\n\n\n\n\n\n\nd <- highlight_key(my_sum) \np <- ggplot(my_sum) +\n  geom_errorbar(\n    aes(x= reorder(RACE,mean), \n        ymin=mean - 2.58*se, \n        ymax=mean + 2.58*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"99% confidence interval of mean maths score by race\")\n\ngg <- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggdist is an R package that provides a flexible set of ggplot2 geoms and stats designed especially for visualising distributions and uncertainty.\nIt is designed for both frequentist and Bayesian uncertainty visualization, taking the view that uncertainty visualization can be unified through the perspective of distribution visualization:\n\nfor frequentist models, one visualises confidence distributions or bootstrap distributions (see vignette(“freq-uncertainty-vis”));\nfor Bayesian models, one visualises probability distributions (see the tidybayes package, which builds on top of ggdist).\n\n\n\n\n\nIn the code chunk below, stat_pointinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %>%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +   #<<\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\nGentle advice: This function comes with many arguments, it is advised to read the syntax reference for more detail.\n\nexam %>%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\nWarning in layer_slabinterval(data = data, mapping = mapping, stat =\nStatPointinterval, : Ignoring unknown parameters: `.point` and `.interval`\n\n\n\n\n\n\n\n\nIn the code chunk below, stat_gradientinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %>%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")\n\nWarning: fill_type = \"gradient\" is not supported by the current graphics device.\n - Falling back to fill_type = \"segments\".\n - If you believe your current graphics device *does* support\n   fill_type = \"gradient\" but auto-detection failed, set that option\n   explicitly and consider reporting a bug.\n - See help(\"geom_slabinterval\") for more information.\n\n\n\n\n\n\n\n\n\nStep 1: Installing ungeviz package\n\ndevtools::install_github(\"wilkelab/ungeviz\")\n\nSkipping install of 'ungeviz' from a github remote, the SHA1 (aeae12b0) has not changed since last install.\n  Use `force = TRUE` to force installation\n\n\nStep 2: Launch the application in R\n\nlibrary(ungeviz)\n\n\n\n\n\nggplot(data = exam, \n       (aes(x = factor(RACE), y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, width = 0.05), \n    linewidth = 0.4, color = \"#0072B2\", alpha = 1/2) +\n  geom_hpline(data = sampler(25, group = RACE), height = 0.6, color = \"#D55E00\") +\n  theme_bw() + \n  # `.draw` is a generated column indicating the sample draw\n  transition_states(.draw, 1, 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_1.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_1.html",
    "title": "Hands-on_Ex04_1",
    "section": "",
    "text": "To gain hands-on experience using:\n\nggstatsplot package to create visual graphics with rich statistical information,\nperformance package to visualise model diagnostics, and\nparameters package to visualise model parameters\n\n\n\n\n\nggstatsplot is an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.\n\nTo provide alternative statistical inference methods by default.\nTo follow best practices for statistical reporting. For all statistical tests reported in the plots, the default template abides by the APA gold standard for statistical reporting. For example, here are results from a robust t-test:\n\n\n\n\n\n\n\n\nIn this exercise, ggstatsplot and tidyverse will be used. Similar to earlier exercises, pacman will be used to load the packages.\n\npacman::p_load(ggstatsplot, tidyverse)\n\n\n\n\n\nexam_data <- read_csv(\"data/Exam_data.csv\", show_col_types = FALSE)\nas_tibble(exam_data)\n\n# A tibble: 322 × 7\n   ID         CLASS GENDER RACE    ENGLISH MATHS SCIENCE\n   <chr>      <chr> <chr>  <chr>     <dbl> <dbl>   <dbl>\n 1 Student321 3I    Male   Malay        21     9      15\n 2 Student305 3I    Female Malay        24    22      16\n 3 Student289 3H    Male   Chinese      26    16      16\n 4 Student227 3F    Male   Chinese      27    77      31\n 5 Student318 3I    Male   Malay        27    11      25\n 6 Student306 3I    Female Malay        31    16      16\n 7 Student313 3I    Male   Chinese      31    21      25\n 8 Student316 3I    Male   Malay        31    18      27\n 9 Student312 3I    Male   Malay        33    19      15\n10 Student297 3H    Male   Indian       34    49      37\n# ℹ 312 more rows\n\n\n\n\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores.\n\nset.seed(1234) #always need to set seed when using bayes stats\n\ngghistostats(\n  data = exam_data,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60, #test the null hypothesis that mean = 60\n  xlab = \"English scores\"\n)\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\n\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThat’s because the Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\nWhen we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10. It can be defined mathematically as:\n\n\nThe Schwarz criterion is one of the easiest ways to calculate rough approximation of the Bayes Factor.\n\n\n\n\nA Bayes Factor can be any positive number. One of the most common interpretations is this one—first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013:\n\n\n\n\nIn the code chunk below, ggbetweenstats() is used to build a visual for two-sample mean test of Maths scores by gender.\n\nggbetweenstats(\n  data = exam_data,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\n\nIn the code chunk below, ggbetweenstats() is used to build a visual for One-way ANOVA test on English score by race.\n\nggbetweenstats(\n  data = exam_data,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that there are multiple options for pairwise display:\n\n“ns” → only non-significant\n“s” → only significant\n“all” → everything\n\n\n\n\n\nBelow is a summary of between-subject tests that can be carried out for each type of analyses:\n\n\n\n\n\n\n\nIn the code chunk below, ggscatterstats() is used to build a visual for Significant Test of Correlation between Maths scores and English scores.\n\nggscatterstats(\n  data = exam_data,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n\nIn the code chunk below, the Maths scores is binned into a 4-class variable by using cut().\n\nexam1 <- exam_data %>% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100))\n)\n\nIn this code chunk below ggbarstats() is used to build a visual for Significant Test of Association\n\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)\n\n\n\n\n\n\n\n\nIn this section, you will learn how to visualise model diagnostic and model parameters by using parameters package.\nToyota Corolla case study will be used. The purpose of study is to build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables.\n\n\n\n\n\n\npacman::p_load(readxl, performance, parameters, see)\n\n\n\n\nIn the code chunk below, read_xls() of readxl package is used to import the data worksheet of ToyotaCorolla.xls workbook into R.\n\ncar_resale <- read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\")\ncar_resale\n\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   <dbl> <chr>    <dbl>     <dbl>     <dbl>    <dbl>  <dbl>         <dbl>  <dbl>\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period <dbl>, HP_Bin <chr>, CC_bin <chr>,\n#   Doors <dbl>, Gears <dbl>, Cylinders <dbl>, Fuel_Type <chr>, Color <chr>,\n#   Met_Color <dbl>, Automatic <dbl>, Mfr_Guarantee <dbl>,\n#   BOVAG_Guarantee <dbl>, ABS <dbl>, Airbag_1 <dbl>, Airbag_2 <dbl>,\n#   Airco <dbl>, Automatic_airco <dbl>, Boardcomputer <dbl>, CD_Player <dbl>,\n#   Central_Lock <dbl>, Powered_Windows <dbl>, Power_Steering <dbl>, …\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that the output object car_resale is a tibble data frame. To read more on tibble data frame, click here.\n\n\n\n\n\n\nThe code chunk below is used to calibrate a multiple linear regression model by using lm() of Base Stats of R.\n\nmodel <- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\nIn the code chunk, check_collinearity() of performance package.\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\ncheck_c <- check_collinearity(model)\nplot(check_c)\n\nVariable `Component` is not in your data frame :/\n\n\n\n\n\n\n\n\nIn the code chunk, check_normality() of performance package.\nNote: Manufacturing year is also removed since it is highly correlated with the age of the car.\n\nmodel1 <- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\ncheck_n <- check_normality(model1)\nplot(check_n)\n\n\n\n\n\n\n\nIn the code chunk, check_heteroscedasticity() of performance package.\n\ncheck_h <- check_heteroscedasticity(model1)\nplot(check_h)\n\n\n\n\n\n\n\nWe can also perform the complete by using check_model().\n\ncheck_model(model1)\n\n\n\n\n\n\n\nIn the code below, plot() of see package and parameters() of parameters package is used to visualise the parameters of a regression model.\n\nplot(parameters(model1))\n\n\n\n\n\n\n\nIn the code below, ggcoefstats() of ggstatsplot package to visualise the parameters of a regression model.\n\nggcoefstats(model1, \n            output = \"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03_1.html",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03_1.html",
    "title": "Hands-on Exercise 3.1",
    "section": "",
    "text": "To create interactive data visualisation by using functions provided by ggiraph and plotlyr packages.\n\n\n\nFirst, we install and lanch the following R Packages.\n\nggiraph: For making ‘ggplot’ graphics interactive.\nplotly: R library for plotting interactive statistical graphs.\nDT: Provides an R interface to the JavaScript library DataTables that create interactive table on html page.\ntidyverse: A family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\npatchwork: For combining multiple ggplot2 graphs into one figure.\n\nSimilar to previous exercises, we will use pacman to install the packages.\n\npacman::p_load(ggiraph, plotly, \n               patchwork, DT, tidyverse) \n\n\n\n\nThe code chunk below read_csv() of readr package is used to import Exam_data.csv data file into R and save it as an tibble data frame called exam_data.\n\nexam_data <- read_csv(\"data/Exam_data.csv\", show_col_types = FALSE)\n\n\n\n\nggiraph is an htmlwidget and a ggplot2 extension. It allows ggplot graphics to be interactive.\nInteractive is made with ggplot geometries that can understand three arguments:\n\nTooltip: a column of data-sets that contain tooltips to be displayed when the mouse is over elements.\nOnclick: a column of data-sets that contain a JavaScript function to be executed when elements are clicked.\nData_id: a column of data-sets that contain an id to be associated with elements. If it used within a shiny application, elements associated with an id (data_id) can be selected and manipulated on client and server sides. Refer to this article for more detail explanation.\n\n\n\nBelow shows a typical code chunk to plot an interactive statistical graph by using ggiraph package. Notice that the code chunk consists of two parts. First, an ggplot object will be created. Next, girafe() of ggiraph will be used to create an interactive svg object.\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)\n\nNotice that two steps are involved. First, an interactive version of ggplot2 geom (i.e. geom_dotplot_interactive()) will be used to create the basic graph. Then, girafe() will be used to generate an svg object to be displayed on an html page.\n\n\n\n\n\n\nInteractivity\n\n\n\nBy hovering the mouse pointer on an data point of interest, the student’s ID will be displayed.\n\n\n\n\n\n\n\n\n\n\n\nThe content of the tooltip can be customised by including a list object as shown in the code chunk below.\n\nexam_data$tooltip <- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS,\n  \"\\n Gender = \", exam_data$GENDER)) \n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618\n)\n\nThe first four lines of codes in the code chunk create a new field called tooltip. At the same time, it populates text in ID and CLASS fields into the newly created field. Next, this newly created field is used as tooltip field as shown in the code of line 7.\n\n\n\n\n\n\nInteractivity\n\n\n\nBy hovering the mouse pointer on an data point of interest, the student’s ID, Class, Gender will be displayed.\n\n\n\n\n\n\n\n\n\n\n\nCode chunk below uses opts_tooltip() of ggiraph to customize tooltip rendering by add css declarations.\n\ntooltip_css <- \"background-color:white; #<<\nfont-style:bold; font-size: 5; color:black;\" #<<\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = ID),                   \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(    #<<\n    opts_tooltip(    #<<\n      css = tooltip_css)) #<<\n)                                        \n\nNotice that the background colour of the tooltip is black and the font colour is white and bold.\n\n\n\n\n\n\nRefer to Customizing girafe objects to learn more about how to customise ggiraph objects.\n\n\n\nCode chunk below shows an advanced way to customise tooltip. In this example, a function is used to compute 90% confident interval of the mean. The derived statistics are then displayed in the tooltip.\n\ntooltip <- function(y, ymax, accuracy = .01) {\n  mean <- scales::number(y, accuracy = accuracy)\n  sem <- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean maths scores:\", mean, \"+/-\", sem)\n}\n\ngg_point <- ggplot(data=exam_data, \n                   aes(x = RACE),\n) +\n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(  \n                     tooltip(y, ymax))),  \n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    fill = \"light blue\"\n  ) +\n  stat_summary(aes(y = MATHS),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, size = 0.2\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\n\nCode chunk below shows the second interactive feature of ggiraph, namely data_id.\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(           \n    aes(data_id = CLASS),             \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n)  \n\n\n\n\n\n\n\nInfo\n\n\n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over.\n\n\n\n\n\n\n\n\nNote that the default value of the hover css is hover_css = “fill=orange”.\n\n\n\nIn the code chunk below, css codes are used to change the highlighting effect.\n\ncss_hover <- girafe_css_bicolor(primary = \"pink\", secondary = \"black\")\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = css_hover),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)  \n\n\n\n\n\n\n\nInfo\n\n\n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over.\n\n\n\n\n\n\n\n\nNote: Different from previous example, in this example the ccs customisation request are encoded directly.\n\n\n\nThere are time that we want to combine tooltip and hover effect on the interactive statistical graph as shown in the code chunk below.\n\ncss_hover <- girafe_css_bicolor(primary = \"pink\", secondary = \"black\")\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS,\n        data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = css_hover),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)  \n\n\n\n\n\n\n\nInfo\n\n\n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over. At the same time, the tooltip will show the CLASS.\n\n\n\n\n\n\n\n\n\n\n\nonclick argument of ggiraph provides hotlink interactivity on the web.\nThe code chunk below shown an example of onclick.\n\nexam_data$onclick <- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618)                                        \n\n\n\n\n\n\n\n\n\n\n\nInfo\n\n\n\nInteractivity: Web document link with a data object will be displayed on the web browser upon mouse click.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that click actions must be a string column in the dataset containing valid javascript instructions.\n\n\n\n\n\nCoordinated multiple views methods has been implemented in the data visualisation below.\n\n\n\n\n\n\nNotice that when a data point of one of the dotplot is selected, the corresponding data point ID on the second data visualisation will be highlighted too.\n\nIn order to build a coordinated multiple views as shown in the example above, the following programming strategy will be used:\nAppropriate interactive functions of ggiraph will be used to create the multiple views. patchwork function of patchwork package will be used inside girafe function to create the interactive coordinated multiple views.\n\n\np1 <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = ID,\n        data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\np2 <- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = ID,\n        data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p1 + p2), \n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       ) \n\nThe data_id aesthetic is critical to link observations between plots and the tooltip aesthetic is optional but nice to have when mouse over a point.\n\n\n\n\nPlotly’s R graphing library create interactive web graphics from ggplot2 graphs and/or a custom interface to the (MIT-licensed) JavaScript library plotly.js inspired by the grammar of graphics. Different from other plotly platform, plot.R is free and open source.\n\nThere are two ways to create interactive graph by using plotly, they are:\nby using plot_ly(),\n\nand by using ggplotly()\n\n\n\nThe tabset below shows an example a basic interactive plot created by using plot_ly().\n\nPlotCode Chunk\n\n\n\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH,\n        type = \"scatter\",\n        mode   = 'markers')\n\n\n\n\n\n\n\nIn the code chunk below, color argument is mapped to a qualitative visual variable (i.e. RACE).\n\nPlotCode Chunk\n\n\n\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH,\n            color = ~RACE,\n        type = \"scatter\",\n        mode   = 'markers')\n\n\n\n\n\n\n\n\n\n\nInteractivity\n\n\n\nClick on the colour symbol at the legend to select data points of specific race.\n\n\n\n\n\nThe code chunk below plots an interactive scatter plot by using ggplotly().\n\nPlotCode Chunk\n\n\n\n\n\n\n\n\n\n\n\np <- ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nggplotly(p)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that the only extra line you need to include in the code chunk is ggplotly().\n\n\n\n\n\nThe creation of a coordinated linked plot by using plotly involves three steps:\n\nhighlight_key() of plotly package is used as shared data.\ntwo scatterplots will be created by using ggplot2 functions.\nlastly, subplot()of plotly package is used to place them next to each other side-by-side.\n\n\nPlotCode Chunk\n\n\n\n\n\n\n\n\n\n\n\nd <- highlight_key(exam_data)\np1 <- ggplot(data=d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 <- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nsubplot(ggplotly(p1),\n        ggplotly(p2))\n\n\n\n\nThing to learn from the code chunk:\n\nhighlight_key() simply creates an object of class crosstalk::SharedData.\nVisit this link to learn more about crosstalk.\n\n\n\n\n\nCrosstalk is an add-on to the htmlwidgets package. It extends htmlwidgets with a set of classes, functions, and conventions for implementing cross-widget interactions (currently, linked brushing and filtering).\n\n\n\nA wrapper of the JavaScript Library DataTables\nData objects in R can be rendered as HTML tables using the JavaScript library ‘DataTables’ (typically via R Markdown or Shiny).\n\n\nDT::datatable(exam_data, class= \"compact\")\n\n\n\n\n\n\n\n\n\n\nPlotCode Chunk\n\n\n\n\nSetting the `off` event (i.e., 'plotly_deselect') to match the `on` event (i.e., 'plotly_selected'). You can change this default via the `highlight()` function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode chunk below is used to implement the coordinated brushing shown above.\n\nd <- highlight_key(exam_data) \np <- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg <- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5) \n\nSetting the `off` event (i.e., 'plotly_deselect') to match the `on` event (i.e., 'plotly_selected'). You can change this default via the `highlight()` function.\n\n\nThings to learn from the code chunk:\n\nhighlight() is a function of plotly package. It sets a variety of options for brushing (i.e., highlighting) multiple plots. These options are primarily designed for linking multiple plotly graphs, and may not behave as expected when linking plotly to another htmlwidget package via crosstalk. In some cases, other htmlwidgets will respect these options, such as persistent selection in leaflet.\nbscols() is a helper function of crosstalk package. It makes it easy to put HTML elements side by side. It can be called directly from the console but is especially designed to work in an R Markdown document. Warning: This will bring in all of Bootstrap!.\n\n\n\n\n\n\n\n\n\n\nThis link provides online version of the reference guide and several useful articles. Use this link to download the pdf version of the reference guide.\n\nHow to Plot With Ggiraph\nInteractive map of France with ggiraph\nCustom interactive sunbursts with ggplot in R\nThis link provides code example on how ggiraph is used to interactive graphs for Swiss Olympians - the solo specialists.\n\n\n\n\n\nGetting Started with Plotly in R\nA collection of plotly R graphs are available via this link.\nCarson Sievert (2020) Interactive web-based data visualization with R, plotly, and shiny, Chapman and Hall/CRC is the best resource to learn plotly for R. The online version is available via this link\nPlotly R Figure Reference provides a comprehensive discussion of each visual representations.\nPlotly R Library Fundamentals is a good place to learn the fundamental features of Plotly’s R API.\nGetting Started\nVisit this link for a very interesting implementation of gganimate by your senior.\nBuilding an animation step-by-step with gganimate.\nCreating a composite gif with multiple gganimate panels"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03_2.html",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03_2.html",
    "title": "Hands-on Exercise 3.2",
    "section": "",
    "text": "When telling a visually-driven data story, animated graphics tends to attract the interest of the audience and make deeper impression than static graphics. In this hands-on exercise, we will:\n\nCreate animated data visualisation by using gganimate and plotly r packages\nReshape data by using tidyr package\nProcess, wrangle and transform data by using dplyr package\n\n\n\nWhen creating animations, the plot does not actually move. Instead, many individual plots are built and then stitched together as movie frames, just like an old-school flip book or cartoon. Each frame is a different plot when conveying motion, which is built using some relevant subset of the aggregate data. The subset drives the flow of the animation when stitched back together.\n\n\n\n\nBefore we dive into the steps for creating an animated statistical graph, it’s important to understand some of the key concepts and terminology related to this type of visualization.\n\nFrame: In an animated line graph, each frame represents a different point in time or a different category. When the frame changes, the data points on the graph are updated to reflect the new data.\nAnimation Attributes: The animation attributes are the settings that control how the animation behaves. For example, you can specify the duration of each frame, the easing function used to transition between frames, and whether to start the animation from the current frame or from the beginning.\n\n\n\n\n\n\n\nTip\n\n\n\nBefore you start making animated graphs, you should first ask yourself: Does it makes sense to go through the effort? If you are conducting an exploratory data analysis, a animated graphic may not be worth the time investment. However, if you are giving a presentation, a few well-placed animated graphics can help an audience connect with your topic remarkably better than static counterparts.\n\n\n\n\n\n\n\n\nFirst, write a code chunk to check, install and load the following R packages:\n\nplotly, R library for plotting interactive statistical graphs.\ngganimate, an ggplot extension for creating animated statistical graphs.\ngifski converts video frames to GIF animations using pngquant’s fancy features for efficient cross-frame palettes and temporal dithering. It produces animated GIFs that use thousands of colors per frame.\ngapminder: An excerpt of the data available at Gapminder.org. We just want to use its country_colors scheme.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\n\n\npacman::p_load(readxl, gifski, gapminder,\n               plotly, gganimate, tidyverse)\n\n\n\n\nIn this hands-on exercise, the Data worksheet from GlobalPopulation Excel workbook will be used.\nWrite a code chunk to import Data worksheet from GlobalPopulation Excel workbook by using appropriate R package from tidyverse family.\n\ncol <- c(\"Country\", \"Continent\")\nglobalPop <- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %>%\n  mutate_each_(funs(factor(.)), col) %>%\n  mutate(Year = as.integer(Year))\n\nWarning: `mutate_each_()` was deprecated in dplyr 0.7.0.\nℹ Please use `across()` instead.\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nread_xls() of readxl package is used to import the Excel worksheet.\nmutate_each_() of dplyr package is used to convert all character data type into factor.\nmutate of dplyr package is used to convert data values of Year field into integer.\n\n\n\n\n\n\n\ngganimate extends the grammar of graphics as implemented by ggplot2 to include the description of animation. It does this by providing a range of new grammar classes that can be added to the plot object in order to customise how it should change with time.\n\ntransition_*() defines how the data should be spread out and how it relates to itself across time.\nview_*() defines how the positional scales should change along the animation.\nshadow_*() defines how data from other points in time should be presented in the given point in time.\nenter_*()/exit_*() defines how new data should appear and how old data should disappear during the course of the animation.\nease_aes() defines how different aesthetics should be eased during transitions.\n\n\n\nIn the code chunk below, the basic ggplot2 functions are used to create a static bubble plot.\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') \n\n\n\n\n\n\n\nIn the code chunk below,\n\ntransition_time() of gganimate is used to create transition through distinct states in time (i.e. Year).\nease_aes() is used to control easing of aesthetics. The default is linear. Other methods are: quadratic, cubic, quartic, quintic, sine, circular, exponential, elastic, back, and bounce.\n\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') +\n  transition_time(Year) +       \n  ease_aes('linear')          \n\nThe animated bubble chart\n\n\n\n\n\n\n\n\n\nIn Plotly R package, both ggplotly() and plot_ly() support key frame animations through the frame argument/aesthetic. They also support an ids argument/aesthetic to ensure smooth transitions between objects with the same id (which helps facilitate object constancy).\n\n\nIn this sub-section, you will learn how to create an animated bubble plot by using ggplotly() method.\n\ngg <- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young')\n\nggplotly(gg)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nAppropriate ggplot2 functions are used to create a static bubble plot.\nThe output is then saved as an R object called gg. ggplotly() is then used to convert the R graphic object into an animated svg object.\n\n\n\n\n\n\nIn this sub-section, you will learn how to create an animated bubble plot by using plot_ly() method.\n\nbp <- globalPop %>%\n  plot_ly(x = ~Old, \n          y = ~Young, \n          size = ~Population, \n          color = ~Continent, \n          frame = ~Year, \n          text = ~Country, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          )\nbp\n\n\n\n\n\n\n\n\n\n\nGetting Started\nVisit this link for a very interesting implementation of gganimate by your senior.\nBuilding an animation step-by-step with gganimate.\nCreating a composite gif with multiple gganimate panels"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "title": "2 Beyond ggplot2 Fundamentals",
    "section": "",
    "text": "The goal of this exercise is to achieve the following:\n\ncontrol the placement of annotation on a graph by using functions provided in ggrepel package,\ncreate professional publication quality figure by using functions provided in ggthemes and hrbrthemes packages,\nplot composite figure by combining ggplot2 graphs by using patchwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#getting-started",
    "title": "2 Beyond ggplot2 Fundamentals",
    "section": "2.2 Getting Started",
    "text": "2.2 Getting Started\n\n2.2.1 Installing and loading the required libraries\nAside from tidyverse, 4 other R packages will be used. There are:\n\nggrepel: an R package provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: an R package provides some extra themes, geoms, and scales for ‘ggplot2’.\nhrbrthemes: an R package provides typography-centric themes and theme components for ggplot2.\npatchwork: an R package for preparing composite figure created using ggplot2.\n\nThe code chunk below uses p_load() of pacman package.\n\npacman::p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               tidyverse) \n\nFor the purpose of this exercise, a data file called Exam_data will be used. It consists of year end examination grades of a cohort of primary 3 students from a local school. It is in csv file format.\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package. readr is one of the tidyverse package.\n\nexam_data <- read_csv(\"data/Exam_data.csv\", show_col_types = FALSE)\n\nThere are a total of seven attributes in the exam_data tibble data frame. Four of them are categorical data type and the other three are in continuous data type.\n\nThe categorical attributes are: ID, CLASS, GENDER and RACE.\nThe continuous attributes are: MATHS, ENGLISH and SCIENCE."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-ggplot2-annotation-ggrepel",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-ggplot2-annotation-ggrepel",
    "title": "2 Beyond ggplot2 Fundamentals",
    "section": "2.3 Beyond ggplot2 Annotation: ggrepel",
    "text": "2.3 Beyond ggplot2 Annotation: ggrepel\nOne of the challenge in plotting statistical graph is annotation, especially with large number of data points.\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  ggtitle(\"English Scores versus Maths scores for Primary 3\") +\n  geom_smooth(method=lm, linewidth=0.5) +\n  geom_label(aes(label = ID), hjust = .5, vjust = -.5) +\n  coord_cartesian(xlim=c(0,100), ylim =c(0,100))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nggrepel is an extension of ggplot2 package which provides geoms for ggplot2 to repel overlapping text as in our example above.\nWe simply replace geom_text() by geom_text_repel() and geom_label() by geom_label_repel.\n\n2.3.1 Working with ggrepel\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  ggtitle(\"English Scores versus Maths scores for Primary 3\") +\n  geom_smooth(formula = y ~ x, method=lm, size=0.5) +\n  geom_label_repel(aes(label = ID), fontface = \"bold\") +\n  coord_cartesian(xlim=c(0,100), ylim =c(0,100))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: ggrepel: 317 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-ggplot2-themes",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-ggplot2-themes",
    "title": "2 Beyond ggplot2 Fundamentals",
    "section": "2.4 Beyond ggplot2 Themes",
    "text": "2.4 Beyond ggplot2 Themes\nggplot2 comes with eight built-in themes, they are: theme_gray(), theme_bw(), theme_classic(), theme_dark(), theme_light(), theme_linedraw(), theme_minimal(), and theme_void().\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_histogram(bins = 20,\n                 boundary = 100,\n                 color = \"grey25\",\n                 fill = \"grey90\") + \n  ggtitle(\"Distribution of Maths Scores in Theme Grey\") +\n  theme_gray()\n\n\n\n\n\n2.4.1 Working with ggtheme package\nggthemes provides ‘ggplot2’ themes that replicate the look of plots by Edward Tufte, Stephen Few, Fivethirtyeight, The Economist, ‘Stata’, ‘Excel’, and The Wall Street Journal, among others.\nIn the example below, The Economist theme is used.\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_histogram(bins = 20,\n                 boundary = 100,\n                 color = \"grey25\",\n                 fill = \"grey90\") + \n  ggtitle(\"Distribution of Maths Scores\") +\n  labs(y= \"Number of Pupils\", x = \"Maths Scores\") +\n  theme_economist()\n\n\n\n\n\n\n2.4.2 Working with hrbthems package\nhrbrthemes package provides a base theme that focuses on typographic elements, including where various labels are placed as well as the fonts that are used.\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  labs(y= \"Number of Pupils\", x = \"Maths Scores\") +\n  theme_ipsum()\n\n\n\n\nHow can we customise our chart? Consult this vignette to learn more.\n::: callout-note\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  labs(y= \"Number of Pupils\", x = \"Maths Scores\") +\n  theme_ipsum(axis_title_size = 18,\n              base_size = 15,\n              grid = \"Y\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\naxis_title_size argument is used to increase the font size of the axis title to 18\nbase_size argument is used to increase the default axis label to 15, and\ngrid argument is used to remove the x-axis grid lines."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-single-graph",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-single-graph",
    "title": "2 Beyond ggplot2 Fundamentals",
    "section": "2.5 Beyond Single Graph",
    "text": "2.5 Beyond Single Graph\nIt is not unusual that multiple graphs are required to tell a compelling visual story. There are several ggplot2 extensions provide functions to compose figure with multiple graphs. In this section, you will learn how to create composite plot by combining multiple graphs. First, let us create three statistical graphics by using the code chunk below.\nGraph 1\n\np1 <- ggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") + \n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of Maths scores\")\np1\n\n\n\n\nGraph 2\n\np2 <- ggplot(data=exam_data, \n             aes(x = ENGLISH)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") + \n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of English scores\")\np2\n\n\n\n\nLastly, we will draw a scatterplot for English score versus Maths score by as shown below:\n\np3 <- ggplot(data=exam_data, \n             aes(x= MATHS, \n                 y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(formula = y ~ x,method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\np3\n\n\n\n\n\n2.5.1 Creating Composite Graphics: pathwork methods\nThere are several ggplot2 extension’s functions support the needs to prepare composite figure by combining several graphs such as grid.arrange() of gridExtra package and plot_grid() of cowplot package. In this section, I am going to shared with you an ggplot2 extension called patchwork which is specially designed for combining separate ggplot2 graphs into a single figure.\nPatchwork package has a very simple syntax where we can create layouts super easily. Here’s the general syntax that combines:\n\nTwo-Column Layout using the Plus Sign +.\nParenthesis () to create a subplot group.\nTwo-Row Layout using the Division Sign /\n\n\n\n2.5.2 Combining two ggplot2 graphs\nFigure in the tabset below shows a composite of two histograms created using patchwork. Note how simple the syntax used to create the plot!\n\np1 + p2\n\n\n\n\n\n\n2.5.3 Combining three ggplot2 graphs\nWe can plot more complex composite by using appropriate operators. For example, the composite figure below is plotted by using:\n\n“|” operator to stack two ggplot2 graphs,\n“/” operator to place the plots beside each other,\n“()” operator the define the sequence of the plotting.\n\n\n(p1 / p2) | p3\n\n\n\n\nTo learn more, refer to Plot Assembly.\n\n\n2.5.4 Creating a composite figure with tag\nIn order to identify subplots in text, patchwork also provides auto-tagging capabilities as shown in the figure below.\n\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\n2.5.5 Creating figure with insert\nBeside providing functions to place plots next to each other based on the provided layout. With inset_element() of patchwork, we can place one or several plots or graphic elements freely on top or below another plot.\n\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.7, \n                   right = 0.5, \n                   top = 0.95)\n\n\n\n\n\n\n2.5.6 Creating a composite figure by using patchwork and ggtheme\nFigure below is created by combining patchwork and theme_economist() of ggthemes package discussed earlier.\n\npatchwork <- (p1 / p2) | p3\npatchwork & theme_economist() +\n  theme(plot.title = element_text(size = 8),\n          axis.title.y = element_text(size = 6),\n          axis.title.x = element_text(size = 6),\n          axis.text.y = element_text(size = 6),\n          axis.text.x = element_text(size = 6))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#reference",
    "title": "2 Beyond ggplot2 Fundamentals",
    "section": "2.6 Reference",
    "text": "2.6 Reference\nPatchwork R package goes nerd viral\nggrepel\nggthemes\nhrbrthemes\nggplot tips: Arranging plots\nggplot2 Theme Elements Demonstration\nggplot2 Theme Elements Reference Sheet"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "Learn the basic principles and essential components of ggplot2\nGain hands-on experience on using these components to plot statistical graphics based on the principle of Layered Grammar of Graphics2\nApply the essential graphical elements provided by ggplot2 to create elegant and yet functional statistical graphics2"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#install-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#install-and-launching-r-packages",
    "title": "Hands-on Exercise 1",
    "section": "1.2.1 Install and launching R packages",
    "text": "1.2.1 Install and launching R packages\nThe code chunk below uses p_load() of pacman package to check if tidyverse packages are installed in the computer. If they are, then they will be launched into R.\n\npacman::p_load(tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-the-data",
    "title": "Hands-on Exercise 1",
    "section": "1.2.2 Importing the data",
    "text": "1.2.2 Importing the data\n\nexam_data <- read.csv(\"data/Exam_data.csv\")\n\n\nYear end examination grades of a cohort of primary 3 students from a local school.\nThere are a total of seven attributes. Four of them are categorical data type and the other three are in continuous data type.\n\nThe categorical attributes are: ID, CLASS, GENDER and RACE.\nThe continuous attributes are: MATHS, ENGLISH and SCIENCE."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#r-graphics-vs-ggplot",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#r-graphics-vs-ggplot",
    "title": "Hands-on Exercise 1",
    "section": "1.3.1 R Graphics VS ggplot",
    "text": "1.3.1 R Graphics VS ggplot\nFirst, let us compare how R Graphics, the core graphical functions of Base R and ggplot plot a simple histogram.\n\nR Graphicsggplot2\n\n\n\nhist(exam_data$MATHS)\n\n\n\n\n\n\n\nggplot(data=exam_data, aes(x = MATHS)) +\n  geom_histogram(bins=10, \n                 boundary = 100,\n                 color=\"black\", \n                 fill=\"grey\") +\n  ggtitle(\"Distribution of Maths scores\")\n\n\n\n\n\n\n\nAs you can see that the code chunk is relatively simple if R Graphics is used. Then, the question is why ggplot2 is recommended?\n\n\n\n\n\n\nImportant\n\n\n\nThe transferable skills from ggplot2 are not the idiosyncrasies of plotting syntax, but a powerful way of thinking about visualisation, as a way of mapping between variables and the visual properties of geometric objects that you can perceive."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#a-layered-grammar-of-graphics",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#a-layered-grammar-of-graphics",
    "title": "Hands-on Exercise 1",
    "section": "1.4.1 A Layered Grammar of Graphics",
    "text": "1.4.1 A Layered Grammar of Graphics\nggplot2 is an implementation of Leland Wilkinson’s Grammar of Graphics. Figure below shows the seven grammars of ggplot2.\n\nReference: Hadley Wickham (2010) “A layered grammar of graphics.” Journal of Computational and Graphical Statistics, vol. 19, no. 1, pp. 3–28.\nA short description of each building block are as follows:\n\nData: The dataset being plotted.\nAesthetics take attributes of the data and use them to influence visual characteristics, such as position, colours, size, shape, or transparency.\nGeometrics: The visual elements used for our data, such as point, bar or line.\nFacets split the data into subsets to create multiple variations of the same graph (paneling, multiple plots).\nStatistics, statiscal transformations that summarise data (e.g. mean, confidence intervals).\nCoordinate systems define the plane on which data are mapped on the graphic.\nThemes modify all non-data components of a plot, such as main title, sub-title, y-aixs title, or legend background."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_bar",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_bar",
    "title": "Hands-on Exercise 1",
    "section": "1.7.1 Geometric Objects: geom_bar",
    "text": "1.7.1 Geometric Objects: geom_bar\nThe code chunk below plots a bar chart by using geom_bar().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_dotplot",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_dotplot",
    "title": "Hands-on Exercise 1",
    "section": "1.7.2 Geometric Objects: geom_dotplot",
    "text": "1.7.2 Geometric Objects: geom_dotplot\nIn a dot plot, the width of a dot corresponds to the bin width (or maximum width, depending on the binning algorithm), and dots are stacked, with each dot representing one observation.\nIn the code chunk below, geom_dotplot() of ggplot2 is used to plot a dot plot.\n\nggplot(data=exam_data, \n       aes(x=MATHS)) +\n  geom_dotplot(dotsize =0.5)\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe y scale is not very useful, in fact it is very misleading.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe code chunk below performs the following two steps:\nscale_y_continuous() is used to turn off the y-axis, and binwidth argument is used to change the binwidth to 2.5.\n\n\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot(binwidth=2.5,         \n               dotsize = 0.5) +      \n  scale_y_continuous(NULL,           \n                     breaks = NULL)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_histogram",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_histogram",
    "title": "Hands-on Exercise 1",
    "section": "1.7.3 Geometric Objects: geom_histogram()",
    "text": "1.7.3 Geometric Objects: geom_histogram()\nIn the code chunk below, geom_histogram() is used to create a simple histogram by using values in MATHS field of exam_data.\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the default bin is 30."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#modifying-a-geometric-object-by-changing-geom",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#modifying-a-geometric-object-by-changing-geom",
    "title": "Hands-on Exercise 1",
    "section": "1.7.4 Modifying a geometric object by changing geom()",
    "text": "1.7.4 Modifying a geometric object by changing geom()\nIn the code chunk below,\n\nbins argument is used to change the number of bins to 20,\nfill argument is used to shade the histogram with light blue color,\nand color argument is used to change the outline colour of the bars in black\n\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20,            \n                 color=\"black\",      \n                 fill=\"light blue\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#modifying-a-geometric-object-by-changing-aes",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#modifying-a-geometric-object-by-changing-aes",
    "title": "Hands-on Exercise 1",
    "section": "1.7.5 Modifying a geometric object by changing aes()",
    "text": "1.7.5 Modifying a geometric object by changing aes()\nThe code chunk below changes the interior colour of the histogram (i.e. fill) by using sub-group of aesthetic().\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           fill = GENDER)) +\n  geom_histogram(bins=20, \n                 color=\"grey30\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis approach can be used to colour, fill and alpha of the geometric."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom-density",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom-density",
    "title": "Hands-on Exercise 1",
    "section": "1.7.6 Geometric Objects: geom-density()",
    "text": "1.7.6 Geometric Objects: geom-density()\ngeom-density() computes and plots kernel density estimate, which is a smoothed version of the histogram.\nIt is a useful alternative to the histogram for continuous data that comes from an underlying smooth distribution.\nThe code below plots the distribution of Maths scores in a kernel density estimate plot.\n\nggplot(data=exam_data,\n       aes(x=MATHS)) +\n  geom_density()\n\n\n\n\nThe code chunk below plots two kernel density lines by using colour or fill arguments of aes()\n\nggplot(data=exam_data,\n       aes(x=MATHS,\n           colour = GENDER)) +\n  geom_density()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_boxplot",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_boxplot",
    "title": "Hands-on Exercise 1",
    "section": "1.7.7 Geometric Objects: geom_boxplot",
    "text": "1.7.7 Geometric Objects: geom_boxplot\ngeom_boxplot() displays continuous value list. It visualises five summary statistics (the median, two hinges and two whiskers), and all “outlying” points individually.\nThe code chunk below plots boxplots by using geom_boxplot().\n\nggplot(data=exam_data,\n       aes(y=MATHS,\n           x=GENDER)) +\n  geom_boxplot()\n\n\n\n\nNotches are used in box plots to help visually assess whether the medians of distributions differ. If the notches do not overlap, this is evidence that the medians are different.\nThe code chunk below plots the distribution of Maths scores by gender in notched plot instead of boxplot.\n\nggplot(data=exam_data,\n       aes(y=MATHS,\n           x=GENDER)) +\n  geom_boxplot(notch=TRUE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_violin",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_violin",
    "title": "Hands-on Exercise 1",
    "section": "1.7.8 Geometric Objects: geom_violin",
    "text": "1.7.8 Geometric Objects: geom_violin\ngeom_violin is designed for creating violin plot. Violin plots are a way of comparing multiple data distributions. With ordinary density curves, it is difficult to compare more than just a few distributions because the lines visually interfere with each other. With a violin plot, it’s easier to compare several distributions since they’re placed side by side.\nThe code below plot the distribution of Maths score by gender in violin plot.\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_violin()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_point",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_point",
    "title": "Hands-on Exercise 1",
    "section": "1.7.9 Geometric Objects: geom_point()",
    "text": "1.7.9 Geometric Objects: geom_point()\ngeom_point() is especially useful for creating scatterplot.\nThe code chunk below plots a scatterplot showing the Maths and English grades of pupils by using geom_point().\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geom-objects-can-be-combined",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geom-objects-can-be-combined",
    "title": "Hands-on Exercise 1",
    "section": "1.7.10 geom objects can be combined",
    "text": "1.7.10 geom objects can be combined\nThe code chunk below plots the data points on the boxplots by using both geom_boxplot() and geom_point().\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot() +                    \n  geom_point(position=\"jitter\", \n             size = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-stat",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-stat",
    "title": "Hands-on Exercise 1",
    "section": "1.8.1 Working with stat()",
    "text": "1.8.1 Working with stat()\nThe boxplots below are incomplete because the positions of the means were not shown.\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-stat---the-stat_summary-method",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-stat---the-stat_summary-method",
    "title": "Hands-on Exercise 1",
    "section": "1.8.2 Working with stat - the stat_summary() method",
    "text": "1.8.2 Working with stat - the stat_summary() method\nThe code chunk below adds mean values by using stat_summary() function and overriding the default geom.\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot() +\n  stat_summary(geom = \"point\",       \n               fun.y=\"mean\",         \n               colour =\"red\",        \n               size=4)\n\nWarning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.\nℹ Please use the `fun` argument instead."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-stat---the-geom-method",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-stat---the-geom-method",
    "title": "Hands-on Exercise 1",
    "section": "1.8.3 Working with stat - the geom() method",
    "text": "1.8.3 Working with stat - the geom() method\nThe code chunk below adding mean values by using geom_() function and overriding the default stat.\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\",        \n             fun.y=\"mean\",           \n             colour =\"red\",          \n             size=4)          \n\nWarning in geom_point(stat = \"summary\", fun.y = \"mean\", colour = \"red\", :\nIgnoring unknown parameters: `fun.y`\n\n\nNo summary function supplied, defaulting to `mean_se()`"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#adding-a-best-fit-curve-on-a-scatterplot",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#adding-a-best-fit-curve-on-a-scatterplot",
    "title": "Hands-on Exercise 1",
    "section": "1.8.4 Adding a best fit curve on a scatterplot?",
    "text": "1.8.4 Adding a best fit curve on a scatterplot?\nThe scatterplot below shows the relationship of Maths and English grades of pupils. The interpretability of this graph can be improved by adding a best fit curve.\n\neval= FALSE\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() \n\n\n\n\nIn the code chunk below, geom_smooth() is used to plot a best fit curve on the scatterplot.\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(size=0.5)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe default method used is loess. Locally estimated scatterplot smoothing, or LOESS, is a nonparametric method for smoothing a series of data in which no assumptions are made about the underlying structure of the data. LOESS uses local regression to fit a smooth curve through a scatterplot of data.\n\n\nThe default smoothing method can be overridden as shown below.\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5)\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-facet_wrap",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-facet_wrap",
    "title": "Hands-on Exercise 1",
    "section": "1.9.1 Working with facet_wrap()",
    "text": "1.9.1 Working with facet_wrap()\nfacet_wrap wraps a 1d sequence of panels into 2d. This is generally a better use of screen space than facet_grid because most displays are roughly rectangular.\nThe code chunk below plots a trellis plot using facet-wrap().\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20) +\n    facet_wrap(~ CLASS)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#facet_grid-function",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#facet_grid-function",
    "title": "Hands-on Exercise 1",
    "section": "1.9.2 facet_grid() function",
    "text": "1.9.2 facet_grid() function\nfacet_grid() forms a matrix of panels defined by row and column facetting variables. It is most useful when you have two discrete variables, and all combinations of the variables exist in the data.\nThe code chunk below plots a trellis plot using facet_grid().\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20) +\n    facet_grid(~ CLASS)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-coordinate",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-coordinate",
    "title": "Hands-on Exercise 1",
    "section": "1.10.1 Working with Coordinate",
    "text": "1.10.1 Working with Coordinate\nBy the default, the bar chart of ggplot2 is in vertical form.\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar()\n\n\n\n\nThe code chunk below flips the horizontal bar chart into vertical bar chart by using coord_flip().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#changing-the-y--and-x-axis-range",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#changing-the-y--and-x-axis-range",
    "title": "Hands-on Exercise 1",
    "section": "1.10.2 Changing the y- and x-axis range",
    "text": "1.10.2 Changing the y- and x-axis range\nThe scatterplot on the right is slightly misleading because the y-aixs and x-axis range are not equal.\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, size=0.5)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe code chunk below fixed both the y-axis and x-axis range from 0-100.\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-theme",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-theme",
    "title": "Hands-on Exercise 1",
    "section": "1.11.1 Working with theme",
    "text": "1.11.1 Working with theme\nThe code chunk below plot a horizontal bar chart using theme_gray().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_gray()\n\n\n\n\nA horizontal bar chart plotted using theme_classic().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_classic()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608-VAA",
    "section": "",
    "text": "Welcome to ISSS608 Visual Analytics and Applications. In this website, you will find my coursework prepared for this course."
  },
  {
    "objectID": "Take-Home_Exercises/Take-Home_Ex01/take-home_Ex01.html",
    "href": "Take-Home_Exercises/Take-Home_Ex01/take-home_Ex01.html",
    "title": "Take Home Exercise 1",
    "section": "",
    "text": "City of Engagement, with a total population of 50,000, is a small city located at Country of Nowhere. The city serves as a service centre of an agriculture region surrounding the city. The local council of the city is in the process of preparing the Local Plan 2023. A sample survey of 1000 representative residents had been conducted to collect data related to their household demographic and spending patterns, among other things.\n\n\n\nIn this take-home exercise, we will apply the concepts and methods learned in Lesson 1-4 to reveal the demographic and financial characteristics of the city of Engagement.\nThe goal is to build effective solutions to help city managers and planners to explore the complex data, to assist with their major community revitalization efforts, including how to allocate a very large city renewal grant they have recently received.\n\n\n\nFor the purpose of this study, two data sets will be analysed. They are:\n\nParticipants.csv: Contains information about the residents of City of Engagement that have agreed to participate in this study.\nFinancialJournal.csv: Contains information about financial transactions of residents in the city.\n\n\n\n\nFirst, the necessary packages are installed and loaded onto RStudio.\n\npacman::p_load(ggplot2, tidyverse, ggrepel, patchwork, \n               ggthemes,hrbrthemes,ggiraph, plotly, \n               patchwork, DT, readxl, gifski, gapminder,\n               gganimate, crosstalk, ggstatsplot, dplyr, lubridate, ggnewscale, broom, knitr, ggridges, viridis, ggdist, ggridges, colorspace)\n\nThe Participants.csv and FinancialJournal.csv data is then imported from csv using readr::read_csv() and saved under “Participants” and “Finance”, respectively.\n\nparticipants <- read_csv(\"data/Participants.csv\", show_col_types = FALSE)\nfinance <- read_csv(\"data/FinancialJournal.csv\", show_col_types = FALSE)\n\n\n\n\nBefore jumping into visualising the data. We first review the data to understand its data structure and clean the data, where necessary, in preparation for visualisation. Looking at the both datasets, a few problems were observed.\n\n\nAs newer versions of readr don’t report the full column specification when data files are loaded. We will use the spec() function to better understand the full column specification:\n\nResults from running the spec() function highlighted that multiple columns were not set to the most appropriate data type; dplyr::mutate will be used to correct this.\n\nparticipantId is in <dbl> format. It should be reformatted to <factor>.\ninterestGroup is in <chr> format. It should be reformatted to <factor>.\neducationLevel is in <chr> format. It should be reformatted to factor, and ordered from low to high.\n\n\n\n\nCode\nspec(participants)\n\nparticipants <- participants %>% mutate_at(c('participantId', 'interestGroup', 'educationLevel'), as.factor)\n\nparticipants$educationLevel <- ordered(participants$educationLevel, levels = c(\"Low\", \"HighSchoolOrCollege\", \"Bachelors\", \"Graduate\"))\n\n\n\nIn order to have the flexibility to analyse the age variable in bands, we will also re-code the age variable into 10 year bands using the cut() function. The new variable is saved under age_band.\n\n\n\nCode\n# Recode ages into 10-year age bands\n# Define breaks and labels\nbreaks <- seq(10, 70, by = 10)\nlabels <- c(\"10-19\", \"20-29\", \"30-39\", \"40-49\", \"50-59\", \"60+\")\n\n# Recode age variable\nparticipants$age_band <- cut(participants$age, breaks = breaks, labels = labels)\n\n\n\n\n\nSimilar to the Participants dataset, multiple issues were observed in the FinancialJournal dataset as well. They were:\n\nparticipantId is in <dbl> format. It should be reformatted to <factor>.\ncategory is in <chr> format. It should be reformatted to <factor>.\n\n\n\nCode\nspec(finance)\nfinance <-finance %>% mutate_at(c('participantId', 'category'), as.factor)\n\n\n\nCurrently, the variable timestamp provides very micro level breakdown of spending pattern; down to the minute. This may not be useful when trying to understanding broad consumption patterns. As such, using lubridate::as.Date the timestamp variable was reformatted to “%Y-%m-%d” and a new variable, MonthYear was created to extract only the year and month data from timestamp.\n\n\n\nCode\nfinance$timestamp <- as.Date(finance$timestamp, format = \"%Y-%m-%d\")\n\n#Extracting month year\nfinance$MonthYear <- format(as.Date(finance$timestamp), \"%Y-%m\")\n\n\n\n1,113 duplicated rows were also observed in the Finance dataset. Using distinct() these rows should be removed from subsequent analyses. The truncated dataset will be saved under fin_new.\n\n\n\nCode\nfin_new <- finance %>% distinct()\n\n\n\nUnder the amount variable in FinancialJournal, inflow and outflow of money transacted is recorded using positive and negative numbers, respectively. This may cause confusion when visualising the data. We will process the data and use the absolute values using abs().\n\n\n\nCode\nfin_new$amount <- abs(fin_new$amount)\n\n\n\nTo ensure completeness of data, we also look at the data at the participant level. From the histogram below, it is clear that a cluster of 131 participants had very few, close to zero transactions. This may be due to participants dropping out from the study, or moving away from the town. Since little to no financial data were collected from these participants, these participants will be removed from subsequent analyses using filter().\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nfin_grouped <- fin_new %>%\n  group_by(participantId) %>%\n  dplyr::summarize(transaction_count = n()) %>%\n  arrange(transaction_count)\n\np <- ggplot(data=fin_grouped, \n       aes(x = transaction_count)) +\n  geom_histogram(bins = 39,\n                color=\"black\",      \n                fill=\"steelblue\") +\n  ggtitle(\"Distribution of Transactions among Participants\") +\n  xlab(\"Transaction Count\") + \n  ylab(\"Number of Participants\") +\n  theme_classic()\n\np <- ggplotly(p, tooltip=c(\"y\"))\n\n# display the plot\np\n\n\n\n\n\n\nCode\n# Find participant IDs in fin_grouped with transaction count < 500\nparticipants_to_remove <- fin_grouped %>%\n  filter(transaction_count < 500) %>%\n  pull(participantId)\n\n# Filter out rows in fin_new for those participants\nfin_new <- fin_new %>%\n  filter(!participantId %in% participants_to_remove)\n\n\n\nCurrently, the data is in a long format, with each row representing 1 transaction. We will transform the data using pivot_wider() to a wide format instead.\n\n\n\nCode\nfin_final <- fin_new %>%\n  group_by(participantId, category, MonthYear) %>%\n  summarise(Total = sum(amount))\n\nfin_final <- pivot_wider(\n  fin_final,names_from = category,values_from =Total)\n\nfin_wide <- fin_final %>%\n  pivot_wider(names_from = MonthYear, values_from = c(Education, Food, Recreation, Shelter, Wage, RentAdjustment))\n\n#convert all NA values to 0 \nfin_wide[is.na(fin_wide)] <- 0\n\n\n\nBefore merging, we will create the following new variables using mutate():\n\nTotal Expenditure across categories, across months e.g., TotExp_Mar\nTotal Earnings, across months e.g., TotEarn_Mar\nTotal Savings, across months calculated using TotEarn - TotExp for each month\n\n\n\n\nCode\n#Total expenditure across months\nfin_wide <- fin_wide %>%\n  mutate(\n    TotExp_Mar = `Education_2022-03` + `Food_2022-03` + `Recreation_2022-03` + `Shelter_2022-03`,\n    TotExp_Apr = `Education_2022-04` + `Food_2022-04` + `Recreation_2022-04` + `Shelter_2022-04`,\n    TotExp_May = `Education_2022-05` + `Food_2022-05` + `Recreation_2022-05` + `Shelter_2022-05`,\n    TotExp_Jun = `Education_2022-06` + `Food_2022-06` + `Recreation_2022-06` + `Shelter_2022-06`,\n    TotExp_Jul = `Education_2022-07` + `Food_2022-07` + `Recreation_2022-07` + `Shelter_2022-07`,\n    TotExp_Aug = `Education_2022-08` + `Food_2022-08` + `Recreation_2022-08` + `Shelter_2022-08`,\n    TotExp_Sep = `Education_2022-09` + `Food_2022-09` + `Recreation_2022-09` + `Shelter_2022-09`,\n    TotExp_Oct = `Education_2022-10` + `Food_2022-10` + `Recreation_2022-10` + `Shelter_2022-10`,\n    TotExp_Nov = `Education_2022-11` + `Food_2022-11` + `Recreation_2022-11` + `Shelter_2022-11`,\n    TotExp_Dec = `Education_2022-12` + `Food_2022-12` + `Recreation_2022-12` + `Shelter_2022-12`,\n    TotExp_Jan = `Education_2023-01` + `Food_2023-01` + `Recreation_2023-01` + `Shelter_2023-01`,\n    TotExp_Feb = `Education_2023-02` + `Food_2023-02` + `Recreation_2023-02` + `Shelter_2023-02`\n)\n\n#Total Earnings across months\nfin_wide <- fin_wide %>%\n  mutate(\n    TotEarn_Mar = `Wage_2022-03` + ifelse(is.na(`RentAdjustment_2022-03`), 0, `RentAdjustment_2022-03`),\n    TotEarn_Apr = `Wage_2022-04` + ifelse(is.na(`RentAdjustment_2022-04`), 0, `RentAdjustment_2022-04`),\n    TotEarn_May = `Wage_2022-05` + ifelse(is.na(`RentAdjustment_2022-05`), 0, `RentAdjustment_2022-05`),\n    TotEarn_Jun = `Wage_2022-06` + ifelse(is.na(`RentAdjustment_2022-06`), 0, `RentAdjustment_2022-06`),\n    TotEarn_Jul = `Wage_2022-07` + ifelse(is.na(`RentAdjustment_2022-07`), 0, `RentAdjustment_2022-07`),\n    TotEarn_Aug = `Wage_2022-08` + ifelse(is.na(`RentAdjustment_2022-08`), 0, `RentAdjustment_2022-08`),\n    TotEarn_Sep = `Wage_2022-09` + ifelse(is.na(`RentAdjustment_2022-09`), 0, `RentAdjustment_2022-09`),\n    TotEarn_Oct = `Wage_2022-10` + ifelse(is.na(`RentAdjustment_2022-10`), 0, `RentAdjustment_2022-10`),\n    TotEarn_Nov = `Wage_2022-11` + ifelse(is.na(`RentAdjustment_2022-11`), 0, `RentAdjustment_2022-11`),\n    TotEarn_Dec = `Wage_2022-12` + ifelse(is.na(`RentAdjustment_2022-12`), 0, `RentAdjustment_2022-12`),\n    TotEarn_Jan = `Wage_2023-01` + ifelse(is.na(`RentAdjustment_2023-01`), 0, `RentAdjustment_2023-01`),\n    TotEarn_Feb = `Wage_2023-02` + ifelse(is.na(`RentAdjustment_2023-02`), 0, `RentAdjustment_2023-02`))\n\n#Total Savings across months\nfin_wide <- fin_wide %>%\n  mutate(\n    TotSav_Mar = TotEarn_Mar - TotExp_Mar,\n    TotSav_Apr = TotEarn_Apr - TotExp_Apr,\n    TotSav_May = TotEarn_May - TotExp_May,\n    TotSav_Jun = TotEarn_Jun - TotExp_Jun,\n    TotSav_Jul = TotEarn_Jul - TotExp_Jul,\n    TotSav_Aug = TotEarn_Aug - TotExp_Aug,\n    TotSav_Sep = TotEarn_Sep - TotExp_Sep,\n    TotSav_Oct = TotEarn_Oct - TotExp_Oct,\n    TotSav_Nov = TotEarn_Nov - TotExp_Nov,\n    TotSav_Dec = TotEarn_Dec - TotExp_Dec,\n    TotSav_Jan = TotEarn_Jan - TotExp_Jan,\n    TotSav_Feb = TotEarn_Feb - TotExp_Feb\n  )\n\n\n\n\n\nLastly, using merge(), both datasets (i.e., participants and fin_wide) will be combined to allow comparisons of financial data between different demographic groups. The new dataset will be saved under final_data.\n\n\nCode\n# merge the datasets\nmerged <- merge(fin_wide, participants, by = \"participantId\", all.x = TRUE)\n\n# subset the merged dataset to keep only the rows with participantId in fin_wide\nfinal_data <- subset(merged, participantId %in% fin_wide$participantId)\n\n\n\n\n\n\nTo have a quick overview of the demographic profile of residents, simple pie charts are used. Specifically, these charts are good when trying to show whole to part relationships, and are useful when there are not too many categories within variables.\n\n\nResidents of the city tended to have small families; no more than 3 members per household. Looking at the two charts side-by-side, we also see that the sample proportion of families with 3 members is the same at the proportion of families with children.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Calculate the proportion of respondents in each HHsize band category\nfinal_data$householdSize <- as.factor(final_data$householdSize)\nhousehold_props <- final_data %>%\n  count(householdSize) %>%\n  mutate(prop = n / sum(n))\n\n# Create a pie chart with the household size\np3 <- ggplot(household_props, aes(x = \"\", y = prop, fill = householdSize)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"white\") +\n  coord_polar(theta = \"y\", start = 0, direction = -1) +\n  scale_fill_manual(values = c(\"pink1\", \"pink3\", \"pink4\")) +\n  theme_void() +\n  theme(legend.position = \"right\",\n        plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5),\n        legend.title = element_text(size = 10, face = \"bold\"),\n        legend.text = element_text(size = 8)) +\n  labs(fill = \"Education Level\") +\n  geom_text(aes(label = paste0(round(prop*100), \"%\")), position = position_stack(vjust = 0.5), size = 3) +\n  ggtitle(\"Residents by Household Size(%)\")\n\n# Calculate the proportion of respondents havekids category\nfinal_data$haveKids <- as.factor(final_data$haveKids)\nkids_props <- final_data %>%\n  count(haveKids) %>%\n  mutate(prop = n / sum(n))\n\n# Create a pie chart using havekids variable\np4 <- ggplot(kids_props, aes(x = \"\", y = prop, fill = haveKids)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"white\") +\n  coord_polar(theta = \"y\", start = 0, direction = -1) +\n  scale_fill_manual(values = c(\"orange\", \"skyblue\")) +\n  theme_void() +\n  theme(legend.position = \"right\",\n        plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5),\n        legend.title = element_text(size = 10, face = \"bold\"),\n        legend.text = element_text(size = 8)) +\n  labs(fill = \"Have Kids\") +\n  geom_text(aes(label = paste0(round(prop*100), \"%\")), position = position_stack(vjust = 0.5), size = 3) +\n  ggtitle(\"Residents by whether they have kids(%)\")\n\n(p3 + p4)\n\n\n\n\n\n\n\nThe age of residents living in the city were relatively evenly distributed, with close to one-third of the population (30%) made up by younger respondents below the ages of 30. Close to half were high school or college educated, while a similar proportion had a bachelor’s degree or higher.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Calculate the proportion of respondents in each age band category\nage_band_props <- final_data %>%\n  count(age_band) %>%\n  mutate(prop = n / sum(n))\n\n# Create a pie chart with the age band proportions\np1 <- ggplot(age_band_props, aes(x = \"\", y = prop, fill = age_band)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"white\") +\n  coord_polar(theta = \"y\", start = 0, direction = -1) +\n  scale_fill_manual(values = c(\"skyblue1\", \"skyblue2\", \"skyblue3\", \"skyblue4\", \"darkslategrey\", \"grey25\")) +\n  theme_void() +\n  theme(legend.position = \"right\",\n        plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5),\n        legend.title = element_text(size = 10, face = \"bold\"),\n        legend.text = element_text(size = 8)) +\n  labs(fill = \"Age Band\") +\n  geom_text(aes(label = paste0(round(prop*100), \"%\")), position = position_stack(vjust = 0.5), size = 3) +\n  ggtitle(\"Residents by Age Band (%)\")\n\n# Calculate the proportion of respondents in each education band category\neducation_props <- final_data %>%\n  count(educationLevel) %>%\n  mutate(prop = n / sum(n))\n\n# Create a pie chart with the age band proportions\np2 <- ggplot(education_props, aes(x = \"\", y = prop, fill = educationLevel)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"white\") +\n  coord_polar(theta = \"y\", start = 0, direction = -1) +\n  scale_fill_manual(values = c(\"darkseagreen1\", \"darkseagreen3\", \"darkseagreen4\", \"darkslategrey\")) +\n  theme_void() +\n  theme(legend.position = \"right\",\n        plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5),\n        legend.title = element_text(size = 10, face = \"bold\"),\n        legend.text = element_text(size = 8)) +\n  labs(fill = \"Education Level\") +\n  geom_text(aes(label = paste0(round(prop*100), \"%\")), position = position_stack(vjust = 0.5), size = 3) +\n  ggtitle(\"Residents by Education Level(%)\")\n\n(p1 + p2)\n\n\n\n\nAs a next step, let’s explore whether there is a relationship between age and education. Since the distribution of age departed from normality, we will use the non-parametric Kruskal-Wallis test. Looking at the boxplots, it was observed that there was no statistically significant correlations between the two variables.\n\n\n\n\n\n\nNote\n\n\n\nThis means that the education levels of younger residents were comparable to older residents. Since education levels are commonly tied to income, this finding may suggest low social mobility among residents in the city. As such, we will explore the relationship between education and wage next.\n\n\n\nPlotCodeNormality TestCode\n\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = final_data,\n  x = educationLevel, \n  y = age,\n  type = \"np\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE) +\n  theme_classic() +\n  theme(plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5),\n       axis.title = element_text(size = 12),\n        axis.text = element_text(size = 7),\n        legend.position = \"none\") +\n  labs(x = \"Education Level\", y = \"Age\") +\n  ggtitle(\"Age by Education Level\")\n\n\n\n\n\n\n\n\n\n\n\n# Calculate Shapiro-Wilk test statistic and p-value\nsw_test <- shapiro.test(final_data$age)\nsw_stat <- sw_test$statistic\nsw_p <- sw_test$p.value\n\nggplot(final_data,\n       aes(sample=age)) +\n  stat_qq() +\n  stat_qq_line() +\n  annotate(\"text\", x = -1.5, y = -2.5, \n           label = paste(\"Shapiro-Wilk test:\", \"\\n\", \"statistic =\", round(sw_stat, 3), \"\\n\", \"p-value =\", format(sw_p, scientific = TRUE, digits = 3)), \n           hjust = 0, vjust = 0, size = 3, color = \"black\")+\n  ggtitle(\"Distribution - Age\") +\n  theme_classic() +\n  theme(plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5),\n       axis.title = element_text(size = 12),\n        axis.text = element_text(size = 10)) +\n  labs(y = \"Age\")\n\n\n\n\n\n\n\nIn order to further explore income, a new variable, mean_wage, was calculated using rowMeans().\n\n\nCode\n# Select the columns that contain wage information\nwage_cols <- grepl(\"^Wage_20\", names(final_data))\n\n# Calculate the average wage\nfinal_data$mean_wage <- rowMeans(final_data[, wage_cols], na.rm = TRUE)\n\n\nBefore running the confirmatory data analysis, aligned with earlier analysis, a normality assumption test for the distribution of average wage among residents was performed. From the qqplot below, we can see that the distribution of average wage significantly departed from normality. Due to this outcome, we will use a non-parametric test.\nLooking at the boxplot, we can see that wage earned by residents were significantly different between education levels. Specifically, residents who received more education, were more likely to earn a higher wage.\n\n\n\n\n\n\nNote\n\n\n\nAs education was found to be positively correlated with wages, there may be value in looking at how residents, especially younger residents, can be nudged to continue their education. Specifically, the local council can seek to understand any barriers towards further education, and interests among citizens.\n\n\n\nPlotCodeNormality TestCode\n\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = final_data,\n  x = educationLevel, \n  y = mean_wage,\n  type = \"np\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE) +\n  theme_classic() +\n  theme(plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5),\n       axis.title = element_text(size = 12),\n        axis.text = element_text(size = 7),\n        legend.position = \"none\") +\n  labs(x = \"Education Level\", y = \"Income\") +\n  ggtitle(\"Income by Education Level\")\n\n\n\n\n\n\n\n\n\n\n\n# Select the columns that contain wage information\nwage_cols <- grepl(\"^Wage_20\", names(final_data))\n\n# Calculate the average wage\nfinal_data$mean_wage <- rowMeans(final_data[, wage_cols], na.rm = TRUE)\n\n# Calculate Shapiro-Wilk test statistic and p-value\nsw_test <- shapiro.test(final_data$mean_wage)\nsw_stat <- sw_test$statistic\nsw_p <- sw_test$p.value\n\nggplot(final_data,\n       aes(sample=mean_wage)) +\n  stat_qq() +\n  stat_qq_line() +\n  annotate(\"text\", x = -1.5, y = -2.5, \n           label = paste(\"Shapiro-Wilk test:\", \"\\n\", \"statistic =\", round(sw_stat, 3), \"\\n\", \"p-value =\", format(sw_p, scientific = TRUE, digits = 3)), \n           hjust = 0.5, vjust = -1.5, size = 3, color = \"black\")+\n  ggtitle(\"Distribution - Wage\") +\n  theme_classic() +\n  theme(plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5),\n       axis.title = element_text(size = 12),\n        axis.text = element_text(size = 10)) +\n  labs(y = \"Wage\")\n\n\n\n\n\n\n\n\n\n\nIn order to chart earnings and expenditure patterns over time. We will transform resident’s earnings and expenditure data into long form using mutate(). A new dataframe named ‘Earn_Exp_avg’ with the following variables will be created:\n\nMonth: Date in ‘YYYY-MM’ format\nEducationLevel: No change from original data\nAvg_Earning: Average monthly earnings i.e., both wage and rentadjustments\nAvg_Expenditure: Average monthly expenditure\nAvg_Savings: Calculated variable deducting monthly expenditure from monthly earnings\n\n\n\nCode\n#Transform Earning Data to long form\nEarn_data_long <- final_data %>%\n  select(participantId, educationLevel, starts_with(\"TotEarn\")) %>%\n  gather(key = \"Month\", value = \"Earning\", starts_with(\"TotEarn\"))\n\nEarn_data_long_renamed <- Earn_data_long %>% \n  mutate(Month = case_when(\n    Month == \"TotEarn_Mar\" ~ \"2022-03\",\n    Month == \"TotEarn_Apr\" ~ \"2022-04\",\n    Month == \"TotEarn_May\" ~ \"2022-05\",\n    Month == \"TotEarn_Jun\" ~ \"2022-06\",\n    Month == \"TotEarn_Jul\" ~ \"2022-07\",\n    Month == \"TotEarn_Aug\" ~ \"2022-08\",\n    Month == \"TotEarn_Sep\" ~ \"2022-09\",\n    Month == \"TotEarn_Oct\" ~ \"2022-10\",\n    Month == \"TotEarn_Nov\" ~ \"2022-11\",\n    Month == \"TotEarn_Dec\" ~ \"2022-12\",\n    Month == \"TotEarn_Jan\" ~ \"2023-01\",\n    Month == \"TotEarn_Feb\" ~ \"2023-02\",\n    TRUE ~ Month\n  )) %>% \n  rename(Month_renamed = Month)\n\nEarn_data_long_renamed$Month <- as.Date(paste0(Earn_data_long_renamed$Month, \"-01\"), format = \"%Y-%m-%d\")\n\nEarn_data_long_renamed <- as_tibble(Earn_data_long_renamed)\n\n#Transform Exp Data to long form\nExp_data_long <- final_data %>%\n  select(participantId, educationLevel, starts_with(\"TotExp\")) %>%\n  mutate(across(starts_with(\"TotExp\"), ~coalesce(., 0))) %>%\n  gather(key = \"Month\", value = \"Expenditure\", starts_with(\"TotExp\"))\n\nExp_data_long_renamed <- Exp_data_long %>% \n  mutate(Month = case_when(\n    Month == \"TotExp_Mar\" ~ \"2022-03\",\n    Month == \"TotExp_Apr\" ~ \"2022-04\",\n    Month == \"TotExp_May\" ~ \"2022-05\",\n    Month == \"TotExp_Jun\" ~ \"2022-06\",\n    Month == \"TotExp_Jul\" ~ \"2022-07\",\n    Month == \"TotExp_Aug\" ~ \"2022-08\",\n    Month == \"TotExp_Sep\" ~ \"2022-09\",\n    Month == \"TotExp_Oct\" ~ \"2022-10\",\n    Month == \"TotExp_Nov\" ~ \"2022-11\",\n    Month == \"TotExp_Dec\" ~ \"2022-12\",\n    Month == \"TotExp_Jan\" ~ \"2023-01\",\n    Month == \"TotExp_Feb\" ~ \"2023-02\",\n    TRUE ~ Month\n  )) %>% \n  rename(Month_renamed = Month)\n\nExp_data_long_renamed$Month <- as.Date(paste0(Earn_data_long_renamed$Month, \"-01\"), format = \"%Y-%m-%d\")\n\nExp_data_long_renamed <- as_tibble(Exp_data_long_renamed)\n#sum(is.na(Exp_data_long_renamed$Expenditure))\n\n# Group and summarize earning data by month\nEarn_data_avg <- Earn_data_long_renamed %>% \n  group_by(Month, educationLevel) %>% \n  summarize(avg_earning = mean(Earning))\n\n# Group and summarize expenditure data by month\nExp_data_avg <- Exp_data_long_renamed %>% \n  group_by(Month, educationLevel) %>% \n  summarize(avg_expenditure = mean(Expenditure))\n\nEarn_Exp_avg <- full_join(Earn_data_avg, Exp_data_avg, \n                           by = c(\"educationLevel\", \"Month\"))\n\n# Create a new column for savings\nEarn_Exp_avg$savings <- Earn_Exp_avg$avg_earning - Earn_Exp_avg$avg_expenditure\n\n#Round values to 2dp\nEarn_Exp_avg$avg_expenditure <- round(Earn_Exp_avg$avg_expenditure, 2)\nEarn_Exp_avg$avg_earning <- round(Earn_Exp_avg$avg_earning, 2)\nEarn_Exp_avg$savings <- round(Earn_Exp_avg$savings, 2)\n\n#Rename Columns \nEarn_Exp_avg <- Earn_Exp_avg %>% rename(Education_Level = educationLevel, Avg_Earning = avg_earning, Avg_Expenditure = avg_expenditure, Avg_Savings = savings)\n\n\nFrom the line charts below, we can see that monthly expenditure among residents tended to be fairly stable across months, regardless of their education levels. Even in the month of March, where earnings tended to spike, expenditure remained fairly low. Notably, instead of spending more, residents with higher wage tended to save more monthly (portion highlighted in yellow).\n\n\n\n\n\n\nNote\n\n\n\nThis suggests that residents, regardless of income, are generally frugal with their money and spend within their means. A good sign in terms of financial health!\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n# Create a ggplot object with the data\np <-ggplot(data = Earn_Exp_avg, aes(x = Month)) +\n  geom_line(aes(y = Avg_Earning, color = \"Avg_Earning\")) +\n  geom_line(aes(y = Avg_Expenditure, color = \"Avg_Expenditure\")) +\n  geom_ribbon(aes(ymin = Avg_Expenditure, ymax = Avg_Earning), fill = \"yellow\", alpha = 0.3) +\n  scale_color_manual(name = NULL, values = c(\"Avg_Earning\" = \"steelblue\", \"Avg_Expenditure\" = \"orange\"), \n                     labels = c(\"Average Earnings\", \"Average Expenditure\")) +\n  labs(title = \"Average Earnings and Expenditure per Month\",\n       x = \"Month\",\n       y = \"Amount\") +\n  theme_classic() +\n  facet_wrap(~ Education_Level, ncol = 1, scales = \"free_y\") +\n  ylim(0, 10000) +\n  scale_x_date(date_labels = \"%b %Y\", date_breaks = \"1 month\") +\n  theme(legend.position = \"bottom\",\n        plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5),\n        axis.title = element_text(size = 10),\n        axis.text = element_text(size = 6))\n\n# Convert the ggplot object to an interactive plotly object and modify the hoverinfo argument\np <- ggplotly(p, height = 500, tooltip = c(\"Avg_Earning\", \"Avg_Expenditure\")) %>%\n  layout(hoverlabel = list(bgcolor = \"white\"))\n\n# Set the legend position to \"bottom\"\np <- layout(p, legend = list(orientation = \"h\", x = 0.25, y = -0.1))\n\n# Display the plot\np\n\n\n\n\n\n\n\nTo better understand spending patterns, using rowMeans(), new variables were calculated to derive the average monthly spending on each category i.e., education, food, recreational, shelter.\n\n\nCode\n# Create New variable for total average education expenditure\nfinal_data$Education <- rowMeans(final_data[c(\"Education_2022-03\", \"Education_2022-04\", \"Education_2022-05\",\"Education_2022-06\", \"Education_2022-07\", \"Education_2022-08\",\"Education_2022-09\", \"Education_2022-10\", \"Education_2022-11\",\"Education_2022-12\", \"Education_2023-01\", \"Education_2023-02\")])\n\n# Create New variable for total average food expenditure\nfinal_data$Food<- rowMeans(final_data[c(\"Food_2022-03\", \"Food_2022-04\", \"Food_2022-05\",\"Food_2022-06\", \"Food_2022-07\", \"Food_2022-08\",\"Food_2022-09\", \"Food_2022-10\", \"Food_2022-11\",\"Food_2022-12\", \"Food_2023-01\", \"Food_2023-02\")])\n\n# Create New variable for total average recreation expenditure\nfinal_data$Recreation<- rowMeans(final_data[c(\"Recreation_2022-03\", \"Recreation_2022-04\", \"Recreation_2022-05\",\"Recreation_2022-06\", \"Recreation_2022-07\", \"Recreation_2022-08\",\"Recreation_2022-09\", \"Recreation_2022-10\", \"Recreation_2022-11\",\"Recreation_2022-12\", \"Recreation_2023-01\", \"Recreation_2023-02\")])\n\n# Create New variable for total average shelter expenditure\nfinal_data$Shelter<- rowMeans(final_data[c(\"Shelter_2022-03\", \"Shelter_2022-04\", \"Shelter_2022-05\",\"Shelter_2022-06\", \"Shelter_2022-07\", \"Shelter_2022-08\",\"Shelter_2022-09\", \"Shelter_2022-10\", \"Shelter_2022-11\",\"Shelter_2022-12\", \"Shelter_2023-01\", \"Shelter_2023-02\")])\n\n# Round the result to 2 decimal places\nfinal_data$Education <- round(final_data$Education, 2)\nfinal_data$Food <- round(final_data$Food, 2)\nfinal_data$Recreation <- round(final_data$Recreation, 2)\nfinal_data$Shelter <- round(final_data$Shelter, 2)\n\n# Calculate the mean spending for each category\nmean_spending <- c(mean(final_data$Education), mean(final_data$Food), \n                   mean(final_data$Recreation), mean(final_data$Shelter))\n\n# Create a data frame with the mean spending for each category\nspending_summary <- data.frame(Category = c(\"Education\", \"Food\", \"Recreation\", \"Shelter\"),\n                               Mean_Spending = mean_spending)\n\n\nFrom the bar chart below, we see that a approximately half of total monthly expenditure was spent on shelter. This was followed by recreational activities, then food and lastly education.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Create a bar chart of the mean spending for each category\nspending_summary %>%\n  mutate(Category = reorder(Category, -Mean_Spending)) %>%\n  ggplot(aes(x = Category, y = Mean_Spending)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  geom_text(aes(label = paste0(\"$\", round(Mean_Spending, 2))), vjust = -0.5) +\n  ggtitle(\"Average Monthly Spending by Category\") +\n  xlab(\"Category\") +\n  ylab(\"Average Monthly Spending\") +\n  theme_classic() +  \n  theme(plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5),\n        axis.title = element_text(size = 10),\n        axis.text = element_text(size = 10))\n\n\n\n\n\n\n\n\n\n\nIt is reasonable to link financial health to happiness. As such, let’s explore the hypothesis that both variables are positively correlated with one another i.e., the wealthier/more savings one has, the happier one is. The scatter plot below however, suggests otherwise.In fact, on the top left corner of the scatter plot, we can see that examples of residents with very high savings indicating low joviality scores.\n\n\n\n\n\n\nNote\n\n\n\nContrary to our hypothesis, monthly savings was found to be negatively correlated with joviality. This may be due to external stressors that come with a higher paying jobs. It may be useful for the local council to reach out to high earners in the city, to better support their well-being.\n\n\n\nPlotCodeStatistical TestCode\n\n\n\n\n\n\n\n\n\n\n\n# Create New variable for Savings\nfinal_data$Avg_Savings <- rowMeans(final_data[, c(\"TotSav_Mar\", \"TotSav_Apr\", \"TotSav_May\",\"TotSav_Jun\", \"TotSav_Jul\", \"TotSav_Aug\",\"TotSav_Sep\", \"TotSav_Oct\", \"TotSav_Nov\",\"TotSav_Dec\", \"TotSav_Jan\", \"TotSav_Feb\")])\n\n# Round the result to 2 decimal places\nfinal_data$Avg_Savings <- round(final_data$Avg_Savings, 2)\n\nplot_ly(data = final_data, \n        x = ~joviality, \n        y = ~Avg_Savings,\n        color = ~educationLevel,\n        type = \"scatter\",\n        mode   = 'markers') %>%\n  add_trace(\n    text = ~paste(\"Joviality: \", joviality, \"<br>\",\n                  \"Avg Savings: $\", Avg_Savings),\n    hoverinfo = \"text\",\n    showlegend = FALSE\n  ) %>%\n  layout(\n    title = \"Correlation between Joviality and Savings by Education\",\n    xaxis = list(title = \"Joviality Score\"),\n    yaxis = list(title = \"Average Savings\"),\n    margin = list(l = 60, r = 10, t = 60, b = 30),\n    plot_bgcolor = \"white\",\n    paper_bgcolor = \"white\",\n    font = list(color = \"black\"),\n    hoverlabel = list(bgcolor = \"white\", font = list(color = \"black\")),\n    legend = list(title = \"Education Level\", font = list(color = \"black\"))\n  )\n\n\n\n\n\n\n\n\n\n\n\nggscatterstats(\n  data = final_data,\n  x = joviality,\n  y = Avg_Savings,\n  marginal = FALSE,\n  ) + \nggtitle(\"Correlation betwen Joviality and Savings\") +\nlabs(x = \"Joviality Score\", y = \"Average Savings\")\n\n\n\n\n\n\n\nAside from financial stability, another possible driver for happiness could be having meaningful engagements outside of work. As such, using expenditure on recreation activities as a proxy, we explore if those who spend more on recreational activities tended to register higher joviality scores.\n\nPlotData\n\n\n\n\n\n\n\n\n\n\nggscatterstats(\n  data = final_data,\n  x = joviality,\n  y = Recreation,\n  marginal = FALSE,\n  ) + \nggtitle(\"Correlation betwen Joviality and Recreation\") +\nlabs(x = \"Joviality Score\", y = \"Recreation Expenditure\")\n\n\n\n\nBreaking down this relationship further by interest groups, we also observe that this positive correlation is true, across all groups.\nAdditional analyses of interest groups by size i.e., number of interested residents, and joviality scores recorded no significant differences.\n\n\n\n\n\n\nNote\n\n\n\nThis finding suggests that taking part and being active in interest group cans lead to a more happier, more fulfilling lives. It is therefore important that the local council look into this aspect e.g, understand the preferences of residents, current infrastructure gaps etc, to better allocate resources and encourage greater participation.\n\n\n\nPlotData\n\n\n\n\n\n\n\n\n\n\nggscatterstats(\n  data = final_data,\n  x = joviality,\n  y = Recreation,\n  marginal = FALSE,\n  ) + \n  ggtitle(\"Correlation between Joviality and Recreation Expenditure by Interest Groups\") +\n  labs(x = \"Joviality Score\", y = \"Recreation Expenditure\") +\n  facet_wrap(~interestGroup)\n\n\n\n\n\n\n\nLastly, we look at the relationship between age and joviality. Looking at the scatter plot below, there appeared to be a slight negative correlation between age and joviality.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggscatterstats(\n  data = final_data,\n  x = age,\n  y = joviality,\n  marginal = FALSE,\n  )\n\n\n\n\nLooking closer at the distribution of joviality scores by 10 year age bands, we also see a larger hump on among residents in their 50s, indicating a score of 0.4 and below. This slope then tapers downards as joviality scores increase.\n\n\n\n\n\n\nNote\n\n\n\nAs older residents were once important contributors to the community, the data suggests that more can be done to understand why older residents are noting lower joviality scores. Taking care of the older residents would also put younger residents at ease, knowing that their parents are well taken care off, and they would be too in the future.\n\n\n\nDistributionCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere is generally a good spread of residents across all ages in the City of Engagement. Families however, tended to be relatively small; no more than 3, with about 1 in 3 household indicating that they have children at home.\nEducation was also largely divided down the middle, with about half of residents holding a bachelors degree and above. Notably, younger respondents were not more likely to be more educated than older residents in the city. As higher education was found to be highly correlated with income, there may be value for the city council to look into nudging younger residents to further their education, to improve their quality of live in the longer term.\nWhen comparing earnings to expenditure, it is noted that the large majority, even those who earned less, tended to spend within their means, indicating good financial health among residents.\nWhile higher income allowed wealthier residents to set aside more savings, the financial stability and freedom did not necessarily lead to to higher joviality scores. On the contrary, residents who registered higher expenditure on recreational activities recorded being happier. It may therefore be useful for the council to strategise allocation of resources to these activities, and nudge greater participation among residents.\nLastly, it was noted that joviality scores tended to slightly decline with age. It may therefore be useful for the local council to reach out to older residents to understand their concerns, in order to better support their golden years!"
  },
  {
    "objectID": "Take-Home_Exercises/Take-Home_Ex01/take-home_Ex01_old.html",
    "href": "Take-Home_Exercises/Take-Home_Ex01/take-home_Ex01_old.html",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "City of Engagement, with a total population of 50,000, is a small city located at Country of Nowhere. The city serves as a service centre of an agriculture region surrounding the city. The main agriculture of the region is fruit farms and vineyards. The local council of the city is in the process of preparing the Local Plan 2023. A sample survey of 1000 representative residents had been conducted to collect data related to their household demographic and spending patterns, among other things. The city aims to use the data to assist with their major community revitalization efforts, including how to allocate a very large city renewal grant they have recently received.\n\n\n\nIn this take-home exercise, you are required to apply the concepts and methods you had learned in Lesson 1-4 to reveal the demographic and financial characteristics of the city of Engagement, using appropriate static and interactive statistical graphics methods.\n\n\n\nFor the purpose of this study, two data sets are provided. They are:\n\nParticipants.csv: Contains information about the residents of City of Engagement that have agreed to participate in this study.\nFinancialJournal.csv: Contains information about financial transactions.\n\n\n\n\nFirst, the necessary packages and datasets are loaded.\n\npacman::p_load(tidyverse, ggrepel, patchwork, \n               ggthemes,hrbrthemes,ggiraph, plotly, \n               patchwork, DT, readxl, gifski, gapminder,\n               gganimate, crosstalk, ggstatsplot, dplyr, lubridate)\n\n\nparticipants <- read_csv(\"data/Participants.csv\", show_col_types = FALSE)\nfinance <- read_csv(\"data/FinancialJournal.csv\", show_col_types = FALSE)\n\n\n\n\nBefore jumping into visualising the data. We first take a deep dive to understand the data structure and clean the data, where necessary, in preparation for visualisation.\n\n\nAs newer versions of readr don’t report the full column specification when data files are loaded. We will use the spec() function to better understand the full column specification:\n\nspec(participants)\n\ncols(\n  participantId = col_double(),\n  householdSize = col_double(),\n  haveKids = col_logical(),\n  age = col_double(),\n  educationLevel = col_character(),\n  interestGroup = col_character(),\n  joviality = col_double()\n)\n\n\nResults from running the spec() function highlighted that multiple columns are not set to the most appropriate data type. For example, participantID should be viewed as a categorical variable, as opposed to a continuous numeric variable. We will use the mutate function to update the data types.\n\nparticipants <- participants %>% mutate_at(c('participantId', 'interestGroup', 'educationLevel'), as.factor)\n\nWe also check the dataset for duplicated participants. Note that we have 1,011 participants in our dataset, instead of 1,000. No duplicates were found.\n\nparticipants %>% distinct(participantId, .keep_all= TRUE)\n\n# A tibble: 1,011 × 7\n   participantId householdSize haveKids   age educationLevel      interestGroup\n   <fct>                 <dbl> <lgl>    <dbl> <fct>               <fct>        \n 1 0                         3 TRUE        36 HighSchoolOrCollege H            \n 2 1                         3 TRUE        25 HighSchoolOrCollege B            \n 3 2                         3 TRUE        35 HighSchoolOrCollege A            \n 4 3                         3 TRUE        21 HighSchoolOrCollege I            \n 5 4                         3 TRUE        43 Bachelors           H            \n 6 5                         3 TRUE        32 HighSchoolOrCollege D            \n 7 6                         3 TRUE        26 HighSchoolOrCollege I            \n 8 7                         3 TRUE        27 Bachelors           A            \n 9 8                         3 TRUE        20 Bachelors           G            \n10 9                         3 TRUE        35 Bachelors           D            \n# ℹ 1,001 more rows\n# ℹ 1 more variable: joviality <dbl>\n\n\n\n\n\nSimilar to the Participants dataset, data cleaning is performed on the finance data to check for appropriate data types and duplicates.\n\nspec(finance)\n\ncols(\n  participantId = col_double(),\n  timestamp = col_datetime(format = \"\"),\n  category = col_character(),\n  amount = col_double()\n)\n\nfinance <-finance %>% mutate_at(c('participantId', 'category'), as.factor)\n\nUnlike the participants data, 1,113 duplicated rows were found in the Finance dataset. These rows will be removed from subsequent analyses.\n\nfinance_new <- finance %>% distinct()\nfinance_new\n\n# A tibble: 1,512,523 × 4\n   participantId timestamp           category  amount\n   <fct>         <dttm>              <fct>      <dbl>\n 1 0             2022-03-01 00:00:00 Wage      2473. \n 2 0             2022-03-01 00:00:00 Shelter   -555. \n 3 0             2022-03-01 00:00:00 Education  -38.0\n 4 1             2022-03-01 00:00:00 Wage      2047. \n 5 1             2022-03-01 00:00:00 Shelter   -555. \n 6 1             2022-03-01 00:00:00 Education  -38.0\n 7 2             2022-03-01 00:00:00 Wage      2437. \n 8 2             2022-03-01 00:00:00 Shelter   -557. \n 9 2             2022-03-01 00:00:00 Education  -12.8\n10 3             2022-03-01 00:00:00 Wage      2367. \n# ℹ 1,512,513 more rows\n\nfinance\n\n# A tibble: 1,513,636 × 4\n   participantId timestamp           category  amount\n   <fct>         <dttm>              <fct>      <dbl>\n 1 0             2022-03-01 00:00:00 Wage      2473. \n 2 0             2022-03-01 00:00:00 Shelter   -555. \n 3 0             2022-03-01 00:00:00 Education  -38.0\n 4 1             2022-03-01 00:00:00 Wage      2047. \n 5 1             2022-03-01 00:00:00 Shelter   -555. \n 6 1             2022-03-01 00:00:00 Education  -38.0\n 7 2             2022-03-01 00:00:00 Wage      2437. \n 8 2             2022-03-01 00:00:00 Shelter   -557. \n 9 2             2022-03-01 00:00:00 Education  -12.8\n10 3             2022-03-01 00:00:00 Wage      2367. \n# ℹ 1,513,626 more rows\n\n\nIn order to look at spending patterns, it maybe useful to look at these trends on a yearly or monhtly basis. As such, new variables were created to extract year and month data from the timestamp.\n\nfinance$timestamp <- as.Date(finance$timestamp, format = \"%Y-%m-%d\")\n\nfinance$Month <- format(as.Date(finance$timestamp), \"%m\")\nfinance$Year <- format(as.Date(finance$timestamp), \"%Y\")\nfinance$Month_Yr <- format(as.Date(finance$timestamp), \"%Y-%m\")\n\nfinance\n\n# A tibble: 1,513,636 × 7\n   participantId timestamp  category  amount Month Year  Month_Yr\n   <fct>         <date>     <fct>      <dbl> <chr> <chr> <chr>   \n 1 0             2022-03-01 Wage      2473.  03    2022  2022-03 \n 2 0             2022-03-01 Shelter   -555.  03    2022  2022-03 \n 3 0             2022-03-01 Education  -38.0 03    2022  2022-03 \n 4 1             2022-03-01 Wage      2047.  03    2022  2022-03 \n 5 1             2022-03-01 Shelter   -555.  03    2022  2022-03 \n 6 1             2022-03-01 Education  -38.0 03    2022  2022-03 \n 7 2             2022-03-01 Wage      2437.  03    2022  2022-03 \n 8 2             2022-03-01 Shelter   -557.  03    2022  2022-03 \n 9 2             2022-03-01 Education  -12.8 03    2022  2022-03 \n10 3             2022-03-01 Wage      2367.  03    2022  2022-03 \n# ℹ 1,513,626 more rows"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex04/In-Class_Ex04.html",
    "href": "In-Class_Ex/In-Class_Ex04/In-Class_Ex04.html",
    "title": "In-Class Exercise 4",
    "section": "",
    "text": "pacman::p_load(rstatix, gt, patchwork, tidyverse)\n\n\nexam_data <- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex04/In-Class_Ex04.html#visualising-normal-distribution",
    "href": "In-Class_Ex/In-Class_Ex04/In-Class_Ex04.html#visualising-normal-distribution",
    "title": "In-Class Exercise 4",
    "section": "2 Visualising Normal Distribution",
    "text": "2 Visualising Normal Distribution\n\nThe plotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data,\n       aes(sample=ENGLISH)) + #aes not taking value of x,y\n  stat_qq() + #calculate if sample is normal \n  stat_qq_line() #plot the theoretical line\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can see that the points deviate significantly from the straight diagonal line. This is a clear indication that the set of data is not normally distributed."
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex04/In-Class_Ex04.html#combining-statistical-graph",
    "href": "In-Class_Ex/In-Class_Ex04/In-Class_Ex04.html#combining-statistical-graph",
    "title": "In-Class Exercise 4",
    "section": "Combining Statistical Graph",
    "text": "Combining Statistical Graph\n\nThe plotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\nqq <- ggplot(exam_data,\n       aes(sample=ENGLISH)) +\n  stat_qq() +\n  stat_qq_line() \n\nsw_t <- exam_data %>%\n  shapiro_test(ENGLISH) %>%\n  gt()\n\ntmp <- tempfile(fileext = '.png')\ngtsave(sw_t, tmp) #save sw_t into temp folder\ntable_png <- png::readPNG(tmp,\n                          native = TRUE)\n\nqq + table_png"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex01/In-class_Ex01.html",
    "href": "In-Class_Ex/In-Class_Ex01/In-class_Ex01.html",
    "title": "In-Class_Ex01",
    "section": "",
    "text": "The code chunk below uses p_load() of pacman package.\n\npacman::p_load(tidyverse)\n\nNext, we import the data.\n\nexam_data <- read.csv(\"data/Exam_data.csv\")\n\n\n\nHorizontal bar chart of students by race\n\nggplot(data=exam_data, \n  aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  ggtitle(\"Students by Race\") +\n  theme_minimal() +\n  theme(\n    panel.background = element_rect(fill = \"lightblue\",\n                                colour = \"lightblue\",\n                                linewidth = 0.5, linetype = \"solid\"),\n    panel.grid.major = element_line(linewidth = 0.5, linetype = 'solid',\n                                colour = \"white\"), \n    panel.grid.minor = element_line(linewidth = 0.25, linetype = 'solid',\n                                colour = \"white\")\n  )"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex01/In-class_Ex01.html#designing-data-driven-graphics-for-analysis-i",
    "href": "In-Class_Ex/In-Class_Ex01/In-class_Ex01.html#designing-data-driven-graphics-for-analysis-i",
    "title": "In-Class_Ex01",
    "section": "2. Designing Data-driven graphics for Analysis I",
    "text": "2. Designing Data-driven graphics for Analysis I\nThe Original Design\nA simple vertical bar chart for frequency analysis.\n\nggplot(data=exam_data, \n  aes(x=RACE)) +\n  geom_bar() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\ny axis lavel is not clear (i.e. count)\nTo support effective comparison, the bars should be sorted by their respective frequencies\nFor statis graph, frequency values should be added to provided additional information\n\n\n\nWith reference to the critics on the earlier slide, a makeover was conducted.\n\nggplot(data=exam_data, \n  aes(x=fct_infreq(RACE))) +\n  geom_bar() +\n  theme_minimal() +\n  ylim(0,220) +\n  labs(y= \"Number of Pupils\", x = \"Race\") +\n  geom_text(stat=\"count\", \n      aes(label=paste0(after_stat(count), \", \", \n      round(after_stat(count)/sum(after_stat(count))*100, 1), \"%\")),\n      vjust=-1)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex01/In-class_Ex01.html#designing-data-driven-graphics-for-analysis-ii",
    "href": "In-Class_Ex/In-Class_Ex01/In-class_Ex01.html#designing-data-driven-graphics-for-analysis-ii",
    "title": "In-Class_Ex01",
    "section": "3. Designing Data-driven graphics for Analysis II",
    "text": "3. Designing Data-driven graphics for Analysis II\nThe Original Design\nA basic histogram with little additional information on the data.\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_histogram(bins = 20)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe following was done to improve the chart: - Adding mean and median lines on the histogram plot - Change fill colour and line colour\n\n\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_histogram(colour = 'darkgrey', fill = 'lightblue',bins = 20) +\n  labs(y= \"Number of Pupils\", x = \"Maths Score\") +\n  geom_vline(aes(xintercept = mean(MATHS, na.rm = T)), \n             col = 'red', \n             linetype = 'longdash', \n             linewidth = 1) + \n  geom_vline(aes(xintercept=median(MATHS, na.rm=T)),\n             col=\"green\",\n             linetype=\"dashed\", \n             linewidth=1)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex01/In-class_Ex01.html#designing-data-driven-graphics-for-analysis-iii",
    "href": "In-Class_Ex/In-Class_Ex01/In-class_Ex01.html#designing-data-driven-graphics-for-analysis-iii",
    "title": "In-Class_Ex01",
    "section": "4. Designing Data-driven graphics for Analysis III",
    "text": "4. Designing Data-driven graphics for Analysis III\nThe original Design\nThe histograms below are elegantly designed, but not informative. More context, such as comparing the scores by gender with all pupuls may provided an added, meaningful perspective.\n\nggplot(data=exam_data, \n       aes(x= ENGLISH)) +\n  geom_histogram() +\n    facet_wrap(~ GENDER)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nAfter some refining.\n\nd <- exam_data   \nd_bg <- d[, -3]  \n\nggplot(d, aes(x = ENGLISH, fill = GENDER)) +\n  geom_histogram(data = d_bg, fill = \"grey\", alpha = .5) +\n  geom_histogram(colour = \"black\") +\n  facet_wrap(~ GENDER) +\n  guides(fill = FALSE) +  \n  theme_bw()\n\nWarning: The `<scale>` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex01/In-class_Ex01.html#designing-data-driven-graphics-for-analysis-vi",
    "href": "In-Class_Ex/In-Class_Ex01/In-class_Ex01.html#designing-data-driven-graphics-for-analysis-vi",
    "title": "In-Class_Ex01",
    "section": "5. Designing Data-driven graphics for Analysis VI",
    "text": "5. Designing Data-driven graphics for Analysis VI\nThe original Design\nA simple scatter plot\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe following was done to improve the chart: - Y axis edited so it is aligned with the x axis - Added dotted guiding lines at the 50 mark point for both axis\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  xlim(0,100) +\n  ylim(0,100) +\n  geom_point() +\n  geom_hline(yintercept=50, colour = 'grey', linetype = 'longdash') + \n  geom_vline(xintercept=50, colour = 'grey', linetype = 'longdash')"
  },
  {
    "objectID": "VAST_Challenge/MC1.html",
    "href": "VAST_Challenge/MC1.html",
    "title": "MC 1",
    "section": "",
    "text": "1 Load the R packages\n\n\nShow the Code\npacman::p_load(jsonlite, tidygraph, ggraph, visNetwork, tidyverse)\n\n\n2 Import data\n\n\nShow the Code\nMC1 <- jsonlite::fromJSON(\"/Users/jiahuiloh/lohjiahui/ISSS608-VAA/VAST_Challenge/MC1/data/MC1.json\")\n\n\n2.1 Extract dataframes and convert to tibble dataframe\n\n\nShow the Code\nMC1_nodes <- as_tibble(MC1$nodes) %>%\n  select(id,type,country) #select fields we want and reorganize the fields moving id forward\n\n\n\n\nShow the Code\nMC1_edges <- as_tibble(MC1$links) %>%\n  select(source, target, type, weight, key)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "title": "Hands on Exercise 5",
    "section": "",
    "text": "In this hands-on exercise, we will learn how to model, analyse and visualise network data using R. By the end of this hands-on exercise, we will be able to:\n\ncreate graph object data frames, manipulate them using appropriate functions of dplyr, lubridate, and tidygraph\nbuild network graph visualisation using appropriate functions of ggraph\ncompute network geometrics using tidygraph\nbuild advanced graph visualisation by incorporating the network geometrics, and\nbuild interactive network visualisation using visNetwork package.\n\n\n\n\n\n\n\nIn this hands-on exercise, four network data modelling and visualisation packages will be installed and launched. They are igraph, tidygraph, ggraph and visNetwork. Beside these four packages, tidyverse and lubridate, an R package specially designed to handle and wrangling time data will be installed and launched too.\n\n\nCode\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)\n\n\n\n\n\nThe data sets used in this hands-on exercise is from an oil exploration and extraction company. There are two data sets. One contains the nodes data and the other contains the edges (also know as link) data.\nGAStech-email_edges.csv: The edges data which consists of two weeks of 9063 emails correspondances between 55 employees\nGAStech_email_nodes.csv: The nodes data which consist of the names, department and title of the 55 employees.\nBoth datasets will be imported into RStudio environment using read_csv() of readr package.\n\n\nCode\nGAStech_nodes <- read_csv(\"data/GAStech_email_node.csv\")\n\n\nRows: 54 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): label, Department, Title\ndbl (1): id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nGAStech_edges <- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\n\nRows: 9063 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): SentDate, Subject, MainSubject, sourceLabel, targetLabel\ndbl  (2): source, target\ntime (1): SentTime\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nNext, we will examine the structure of the data frame using glimpse() of dplyr.\n\n\nCode\nglimpse(GAStech_nodes)\n\n\nRows: 54\nColumns: 4\n$ id         <dbl> 1, 2, 3, 4, 5, 6, 7, 44, 45, 46, 8, 9, 10, 11, 12, 13, 14, …\n$ label      <chr> \"Mat.Bramar\", \"Anda.Ribera\", \"Rachel.Pantanal\", \"Linda.Lago…\n$ Department <chr> \"Administration\", \"Administration\", \"Administration\", \"Admi…\n$ Title      <chr> \"Assistant to CEO\", \"Assistant to CFO\", \"Assistant to CIO\",…\n\n\n\n\nCode\nglimpse(GAStech_edges)\n\n\nRows: 9,063\nColumns: 8\n$ source      <dbl> 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      <dbl> 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    <chr> \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    <time> 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     <chr> \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject <chr> \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel <chr> \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel <chr> \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe output report of GAStech_edges above reveals that the SentDate is treated as “Character” data type instead of date data type. This is an error! Before we continue, it is important for us to change the data type of SentDate field back to “Date”” data type.\n\n\n\n\n\n\n\nLet’s correct the data type for date.\n\n\nCode\nGAStech_edges <- GAStech_edges %>%\n  mutate(SendDate = dmy(SentDate)) %>%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nboth dmy() and wday() are functions of lubridate package. lubridate is an R package that makes it easier to work with dates and times.\ndmy() transforms the SentDate to Date data type.\nwday() returns the day of the week as a decimal number or an ordered factor if label is TRUE. The argument abbr is FALSE keep the data spelling in full, i.e. Monday. The function will create a new column in the data.frame i.e. Weekday and the output of wday() will save in this newly created field.\nthe values in the Weekday field are in ordinal scale.\n\n\n\nA quick review of the data using glimspe() shows the data structure of the reformatted GAStech_edges data frame.\n\n\nCode\nglimpse(GAStech_edges)\n\n\nRows: 9,063\nColumns: 10\n$ source      <dbl> 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      <dbl> 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    <chr> \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    <time> 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     <chr> \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject <chr> \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel <chr> \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel <chr> \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n$ SendDate    <date> 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-0…\n$ Weekday     <ord> Friday, Friday, Friday, Friday, Friday, Friday, Friday, Fr…\n\n\n\n\n\nA close examination of GAStech_edges data.frame reveals that it consists of individual e-mail flow records. This is not very useful for visualisation.\nIn view of this, we will aggregate the individual by date, senders, receivers, main subject and day of the week.\n\n\nCode\nGAStech_edges_aggregated <- GAStech_edges %>%\n  filter(MainSubject == \"Work related\") %>%\n  group_by(source, target, Weekday) %>%\n    summarise(Weight = n()) %>%\n  filter(source!=target) %>%\n  filter(Weight > 1) %>%\n  ungroup()\n\n\n`summarise()` has grouped output by 'source', 'target'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nfour functions from dplyr package are used. They are: filter(), group(), summarise(), and ungroup().\nThe output data.frame is called GAStech_edges_aggregated.\nA new field called Weight has been added in GAStech_edges_aggregated.\n\n\n\nTable below shows the data structure of the reformatted GAStech_edges data frame.\n\nglimpse(GAStech_edges_aggregated)\n\nRows: 1,372\nColumns: 4\n$ source  <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ target  <dbl> 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6,…\n$ Weekday <ord> Sunday, Monday, Tuesday, Wednesday, Friday, Sunday, Monday, Tu…\n$ Weight  <int> 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5,…\n\n\n\n\n\n\nIn this section, you will learn how to create a graph data model by using tidygraph package. It provides a tidy API for graph/network manipulation. While network data itself is not tidy, it can be envisioned as two tidy tables, one for node data and one for edge data. tidygraph provides a way to switch between the two tables and provides dplyr verbs for manipulating them. Furthermore it provides access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow.\nBefore getting started, you are advised to read these two articles:\n\n-   [Introducing tidygraph](https://www.data-imaginist.com/2017/introducing-tidygraph/)\n-   [tidygraph 1.1 - A tidy hope](https://www.data-imaginist.com/2018/tidygraph-1-1-a-tidy-hope/)\n\n\nTwo functions of tidygraph package can be used to create network objects, they are:\n\ntbl_graph() creates a tbl_graph network object from nodes and edges data.\nas_tbl_graph() converts network data and objects to a tbl_graphnetwork. Below are network data and objects supported by as_tbl_graph()\n\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).\n\n\n\n\n\n\nactivate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\n\n\n\nIn the above the .N() function is used to gain access to the node data while manipulating the edge data. Similarly .E() will give you the edge data and .G() will give you the tbl_graph object itself.\n\n\n\n\nIn this section, you will use tbl_graph() of tinygraph package to build an tidygraph’s network graph data.frame.\nBefore typing the codes, you are recommended to review to reference guide of tbl_graph()\n\nGAStech_graph <- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\n\n\n\nCode\nGAStech_graph\n\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# A tibble: 54 × 4\n     id label               Department     Title                                \n  <dbl> <chr>               <chr>          <chr>                                \n1     1 Mat.Bramar          Administration Assistant to CEO                     \n2     2 Anda.Ribera         Administration Assistant to CFO                     \n3     3 Rachel.Pantanal     Administration Assistant to CIO                     \n4     4 Linda.Lagos         Administration Assistant to COO                     \n5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Manag…\n6     6 Carla.Forluniau     Administration Assistant to IT Group Manager        \n# ℹ 48 more rows\n#\n# A tibble: 1,372 × 4\n   from    to Weekday Weight\n  <int> <int> <ord>    <int>\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 4541 edges.\nThe command also prints the first six rows of “Node Data” and the first three of “Edge Data”.\nIt states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\n\n\n\n\n\n\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest “weight” first, we could use activate() and then arrange().\n\n\nCode\nGAStech_graph %>%\n  activate(edges) %>%\n  arrange(desc(Weight))\n\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# A tibble: 1,372 × 4\n   from    to Weekday  Weight\n  <int> <int> <ord>     <int>\n1    40    41 Saturday     13\n2    41    43 Monday       11\n3    35    31 Tuesday      10\n4    40    41 Monday       10\n5    40    43 Monday       10\n6    36    32 Sunday        9\n# ℹ 1,366 more rows\n#\n# A tibble: 54 × 4\n     id label           Department     Title           \n  <dbl> <chr>           <chr>          <chr>           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows\n\n\nVisit the reference guide of activate() to find out more about the function.\n\n\n\n\nggraph is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs.\nAs in all network graph, there are three main aspects to a ggraph’s network graph, they are:\n\nnodes,\nedges and\nlayouts.\n\n\n\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph. Before your get started, it is advisable to read their respective reference guide at least once.\n\n\nCode\nggraph(GAStech_graph) +\n  geom_edge_link(edgewidth = 0.1, edge_alpha = 0.5) +\n  geom_node_point()\n\n\nUsing \"stress\" as default layout\n\n\nWarning in geom_edge_link(edgewidth = 0.1, edge_alpha = 0.5): Ignoring unknown\nparameters: `edgewidth`\n\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nThe basic plotting function is ggraph(), which takes the data to be used for the graph and the type of layout desired. Both of the arguments for ggraph() are built around igraph. Therefore, ggraph() can use either an igraph object or a tbl_graph object.\n\n\n\n\n\n\n\nIn this section, you will use theme_graph() to remove the x and y axes. Before your get started, it is advisable to read it’s reference guide at least once.\n\n\nCode\ng <- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\n\nUsing \"stress\" as default layout\n\n\nCode\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nggraph introduces a special ggplot theme that provides better defaults for network graphs than the normal ggplot defaults. theme_graph(), besides removing axes, grids, and border, changes the font to Arial Narrow (this can be overridden).\nThe ggraph theme can be set for a series of plots with the set_graph_style() command run before the graphs are plotted or by using theme_graph() in the individual plots.\n\n\n\n\n\nFurthermore, theme_graph() makes it easy to change the coloring of the plot.\n\n\nCode\ng <- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\n\nUsing \"stress\" as default layout\n\n\nCode\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n\n\nggraph support many layout for standard used, they are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl. Figures below and on the right show layouts supported by ggraph().\n\n\n\n\n\nThe code chunks below will be used to plot the network graph using Fruchterman and Reingold layout.\n\n\nCode\ng <- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThing to learn from the code chunk above: - layout argument is used to define the layout to be used.\n\n\n\n\n\nIn this section, you will colour each node by referring to their respective departments.\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\ngeom_node_point is equivalent in functionality to geo_point of ggplot2. It allows for simple plotting of nodes in different shapes, colours and sizes. In the codes chnuks above colour and size are used.\n\n\n\n\n\n\nIn the code chunk below, the thickness of the edges will be mapped with the Weight variable.\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\ngeom_edge_link draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. In the example above, argument width is used to map the width of the line in proportional to the Weight attribute and argument alpha is used to introduce opacity on the line.\n\n\n\n\n\n\nAnother very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaningful way by spreading nodes and edges out based on their attributes. In this section, you will learn how to use faceting technique to visualise network data.\nThere are three functions in ggraph to implement faceting, they are:\n\nfacet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here,\nfacet_edges() whereby nodes are always drawn in a panels even if the node data contains an attribute named the same as the one used for the edge facetting, and\nfacet_graph() faceting on two variables simultaneously.\n\n\n\nIn the code chunk below, facet_edges() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\nThe code chunk below uses theme() to change the position of the legend.\n\nset_graph_style()\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\nThe code chunk below adds frame to each graph.\n\nset_graph_style() \n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\nIn the code chunkc below, facet_nodes() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\n\n\n\nCentrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures, namely: degree, betweenness, closeness and eigenvector. It is beyond the scope of this hands-on exercise to cover the principles and mathematics of these measure here. Students are encouraged to refer to Chapter 7: Actor Prominence of A User’s Guide to Network Analysis in R to gain better understanding of theses network measures.\n\ng <- GAStech_graph %>%\n  mutate(betweenness_centrality = centrality_betweenness()) %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\nThings to learn from the code chunk above:\n\nmutate() of dplyr is used to perform the computation.\nthe algorithm used, on the other hand, is the centrality_betweenness() of tidygraph.\n\n\n\n\nIt is important to note that from ggraph v2.0 onward tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\n\ng <- GAStech_graph %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. Use this link to find out more about community detection functions provided by tidygraph,\nIn the code chunk below group_edge_betweenness() is used.\n\ng <- GAStech_graph %>%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `community = as.factor(group_edge_betweenness(weights = Weight,\n  directed = TRUE))`.\nCaused by warning in `cluster_edge_betweenness()`:\n! At core/community/edge_betweenness.c:493 : Membership vector will be selected based on the highest modularity score.\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\nvisNetwork() is a R package for network visualization, using vis.jsjavascript library.\nvisNetwork() function uses a nodes list and edges list to create an interactive graph.\n\nThe nodes list must include an “id” column, and the edge list must have “from” and “to” columns.\nThe function also plots the labels for the nodes, using the names of the actors from the “label” column in the node list.\n\nThe resulting graph is fun to play around with.\n\nYou can move the nodes and the graph will use an algorithm to keep the nodes properly spaced.\nYou can also zoom in and out on the plot and move it around to re-center it.\n\n\n\n\nBefore we can plot the interactive network graph, we need to prepare the data model by using the code chunk below.\n\nGAStech_edges_aggregated <- GAStech_edges %>%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %>%\n  rename(from = id) %>% \n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %>%\n  rename(to = id) %>% \n  filter(MainSubject == \"Work related\") %>%\n  group_by(from, to) %>%\n    summarise(weight = n()) %>%\n  filter(from!=to) %>%\n  filter(weight > 1) %>%\n  ungroup()\n\n`summarise()` has grouped output by 'from'. You can override using the\n`.groups` argument.\n\n\n\n\n\nThe code chunk below will be used to plot an interactive network graph by using the data prepared.\n\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n\n\n\n\n\nIn the code chunk below, Fruchterman and Reingold layout is used.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\nVisit Igraph to find out more about visIgraphLayout’s argument.\n\n\n\nvisNetwork() looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field.\nThe code chunk below rename Department field to group.\n\nGAStech_nodes <- GAStech_nodes %>%\n  rename(group = Department) \n\nWhen we rerun the code chunk below, visNetwork shades the nodes by assigning unique colour to each category in the group field.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\nIn the code run below visEdges() is used to symbolise the edges. - The argument arrows is used to define where to place the arrow. - The smooth argument is used to plot the edges using a smooth curve.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\nIn the code chunk below, visOptions() is used to incorporate interactivity features in the data visualisation. - The argument highlightNearest highlights nearest when clicking a node. - The argument nodesIdSelection adds an id node selection creating an HTML select element.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nVisit Option to find out more about visOption's argument."
  },
  {
    "objectID": "Take-Home_Exercises/Take-Home_Ex02/take-home_Ex02.html",
    "href": "Take-Home_Exercises/Take-Home_Ex02/take-home_Ex02.html",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "Seafood is one of the most widely traded commodities in the global food market. More than a third of the world’s population relies on fish and other seafood as a primary source of protein in their diet, and an estimated 520 million people make their livelihoods through fishing or fishing-related activities.\nUnfortunately, illegal, unreported, and unregulated fishing is a major contributor to over fishing worldwide. These activities pose a threat not only to fragile marine ecosystems, but also to food security in coastal communities and regional stability more broadly. The illegal fishing trade has been linked to organized crime, and human rights violations are common when fishing operations are conducted without regulatory oversight.\nThe country of Oceanus has sought FishEye International’s help in identifying companies possibly engaged in illegal, unreported, and unregulated (IUU) fishing. Using visual analytics, this challenge will sought to help FishEye identify companies that may be engaged in illegal fishing.\n\n\nThe data used for this exercises will be from Mini Challenge 2 of the VAST Challenge. This includes 2 main files; the first dataset in json format consists of 34,552 nodes and 5,464,092 directed edges, while the second set of files, a bundle of 12, each represent the output of an AI program for link reference Each bundle represents a list of potential edges to add to the main graph. Details of the attributes provided are listed below:\nNode:\n\nid: Name of the company that originated (or received) the shipment\nshpcountry: Country the company most often associated with when shipping\nrcvcountry: Country the company most often associated with when receiving\ndataset: Always ‘MC2’\n\nEdge:\n\narrivaldate: Date the shipment arrived at port in YYYY-MM-DD format.\nhscode: Harmonized System code for the shipment. Can be joined with the hscodes table to get additional details.\nvalueofgoods_omu: Customs-declared value of the total shipment, in Oceanus Monetary Units (OMU)\nvolumeteu: The volume of the shipment in ‘Twenty-foot equivalent units’, roughly how many 20-foot standard containers would be required. (Actual number of containers may have been different as there are 20ft and 40ft standard containers and tankers that do not use containers)\nweightkg: The weight of the shipment in kilograms (if known)\ndataset: Always ‘MC2’\ntype: Always ‘shipment’ for MC2\ngenerated_by: Name of the program that generated the edge. (Only found on ‘bundle’ records.)\n\n\n\n\nThe main objective of this exercise is to use visual analytics to identify temporal patterns for individual entities and between entities in the knowledge graph FishEye created from trade records, and categorize the types of business relationship patterns.\nSpecifically, this piece will take a different lens and zoom into outlier companies with exceptionally high shipment weights, to try to uncover business patterns."
  },
  {
    "objectID": "Take-Home_Exercises/Take-Home_Ex02/take-home_Ex02.html#load-and-import-data",
    "href": "Take-Home_Exercises/Take-Home_Ex02/take-home_Ex02.html#load-and-import-data",
    "title": "Take Home Exercise 2",
    "section": "2 Load and Import Data",
    "text": "2 Load and Import Data\n\n2.1 Load R Packages\nFirst, the necessary packages are installed and loaded onto the RStudio environment.\n\n\nShow the Code\npacman::p_load(jsonlite, tidygraph, ggraph, visNetwork, tidyverse, igraph, ggraph, lubridate, clock, graphlayouts, ggridges, viridis, ggdist, colorspace, ggdist, ggridges, ggthemes, colorspace, scales, cowplot, visNetwork, plotly, stringr, igraph, patchwork, viridis)\n\n\n\n\n2.2 Importing the data\nUnlike earlier exercises where the data files were in csv format, the data files provided in the challenge are in json format. As such, the dataset will he imported using jsonlite::fromJSON and saved as ‘MC2’.\n\n\nShow the Code\nMC2 <- jsonlite::fromJSON(\"/Users/jiahuiloh/lohjiahui/ISSS608-VAA/Take-Home_Exercises/Take-Home_Ex02/data/mc2_challenge_graph.json\")\n\n\nNext we will convert the file into tibble format using as_tibble. The ‘nodes’ list will be saved as’MC2_nodes’, and ’links’ list as ’MC2_edges’. The variables in the lists will also be re-ordered to facilitate subsequent visualization tasks. The details are as follows:\nMC2_nodes: id, shpcountry, rcvcountry\nMC2_edges: source, target,arrivaldate, hscode, weightkg, valueofgoods_omu, valueofgoodsusd, volumeteu\nFor both, ‘dataset’ column is dropped since it does not provide any additional information or insights.\n\n\nShow the Code\nMC2_nodes <- as_tibble(MC2$nodes) %>%\n  select(id,shpcountry,rcvcountry) \n\nMC2_edges <- as_tibble(MC2$links) %>%\n  select(source, target, arrivaldate, hscode, weightkg, valueofgoods_omu, valueofgoodsusd, volumeteu)"
  },
  {
    "objectID": "Take-Home_Exercises/Take-Home_Ex02/take-home_Ex02.html#data-review-and-wrangling",
    "href": "Take-Home_Exercises/Take-Home_Ex02/take-home_Ex02.html#data-review-and-wrangling",
    "title": "Take Home Exercise 2",
    "section": "3 Data Review and Wrangling",
    "text": "3 Data Review and Wrangling\nAfter a quick exploration of the data, Looking a few problems were observed:\n\narrivaldate in <chr> format, should be transformed to <date> format\nlarge proportions of missing data, recorded as ‘NA’ or ‘0’ under valueofgoods_omu, valueofgoodsusd and volumeteu.\nmultiple duplicated rows in the edges list\nirrelevant hscodes i.e., not associated with the fishing industry\n\nThe following subsections will document the cleaning process.\n\n3.1 Correcting for ‘date’ format\nUsing the glimpse() function from the dplyr library, we review the columns in the new dataframes.\n\n\nShow the Code\nglimpse(MC2_nodes)\n\n\nRows: 34,576\nColumns: 3\n$ id         <chr> \"AquaDelight Inc and Son's\", \"BaringoAmerica Marine Ges.m.b…\n$ shpcountry <chr> \"Polarinda\", NA, \"Oceanus\", NA, \"Oceanus\", \"Kondanovia\", NA…\n$ rcvcountry <chr> \"Oceanus\", NA, \"Oceanus\", NA, \"Oceanus\", \"Utoporiana\", NA, …\n\n\nShow the Code\nglimpse(MC2_edges)\n\n\nRows: 5,464,378\nColumns: 8\n$ source           <chr> \"AquaDelight Inc and Son's\", \"AquaDelight Inc and Son…\n$ target           <chr> \"BaringoAmerica Marine Ges.m.b.H.\", \"BaringoAmerica M…\n$ arrivaldate      <chr> \"2034-02-12\", \"2034-03-13\", \"2028-02-07\", \"2028-02-23…\n$ hscode           <chr> \"630630\", \"630630\", \"470710\", \"470710\", \"470710\", \"47…\n$ weightkg         <int> 4780, 6125, 10855, 11250, 11165, 11290, 9000, 19490, …\n$ valueofgoods_omu <dbl> 141015, 141015, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ valueofgoodsusd  <dbl> NA, NA, NA, NA, NA, NA, 87110, 188140, NA, 221110, 58…\n$ volumeteu        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe output report of ‘MC2_edges’ above reveals that the ‘arrivaldate’ is treated as ‘Character’ data type instead of date data type. This is an error! Before we continue, it is important for us to change the data type of ‘arrivaldate’ field back to ‘Date’ data type.\n\n\nTo correct the data type for ‘arrivaldate’ we will use the ymd() function of lubridate package. Additionally, we will create 3 new columns to extract the year month, year and month data from ‘arrivaldate’. Doing so will allow us to look at broader and seasonal patterns, if any, in subsequent analyses.\n\n\nShow the Code\nMC2_edges <- MC2_edges %>%\n  mutate(arrivaldate = as.Date(arrivaldate),\n         yearmonth = format(arrivaldate, \"%Y-%m\"),\n         year = year(arrivaldate),\n         month = factor(month.abb[month(arrivaldate)], levels = month.abb, ordered = TRUE))\n\n\n\n\n3.2 Completeness of data provided\nTo assess which variables may be most useful for analysis, we use the summarise_all() function to review each variable to understand how complete the data provided is.\nMC2_node:\n\nThere are 0 missing values in the ID column.\nMore than half of the rows in ‘shpcountry’ were recorded as NA, suggesting that we should we decide to use this data column, it should be used with caution.\n\n\n\nShow the Code\nMC2_nodes %>%\n  summarise_all(~ sum(is.na(.)) / n())\n\n\n# A tibble: 1 × 3\n     id shpcountry rcvcountry\n  <dbl>      <dbl>      <dbl>\n1     0      0.647     0.0841\n\n\nMC2_edges:\n\nLooking at the two variables indicating value of goods in both omu and usd, a high proportion of NA was recorded.\nApproximately 84% of records in the ‘volumeteu’ column was noted as 0, and may not be very useful for analyses.\nNo missing value, however, was noted for the weightkg column. It may therefore be more efficient to use this column for analyses.\n\n\n\nShow the Code\nMC2_edges %>%\n  summarise_all(~ sum(is.na(.)) / n())\n\n\n# A tibble: 1 × 11\n  source target arrivaldate hscode weightkg valueofgoods_omu valueofgoodsusd\n   <dbl>  <dbl>       <dbl>  <dbl>    <dbl>            <dbl>           <dbl>\n1      0      0           0      0        0             1.00           0.552\n# ℹ 4 more variables: volumeteu <dbl>, yearmonth <dbl>, year <dbl>, month <dbl>\n\n\nShow the Code\nMC2_edges %>%\n  summarise(zero_proportion = mean(volumeteu == 0, na.rm = TRUE))\n\n\n# A tibble: 1 × 1\n  zero_proportion\n            <dbl>\n1           0.842\n\n\n\n\n\n\n\n\nNote\n\n\n\nWith this summary in mind, as mentioned in the objectives, analyses would focus on using the weight of shipments to understand trade relationships between companies.\n\n\n\n\n3.3 Checking for duplicates\nAs both datasets are relatively large. We will also check for duplicates in both dataframes using the distinct() function from dplyr.\nMC2_nodes: no duplicated rows were found.\nMC2_edges: 155,291 duplicated rows were identified\n\n\nShow the Code\n#Nodes\n# Remove duplicated rows and keep only unique rows\nMC2_nodes_unique <- distinct(MC2_nodes)\n\n# Calculate number of removed duplicated rows\nnum_removed_rows <- nrow(MC2_nodes) - nrow(MC2_nodes_unique)\n\n# Print the number of duplicated rows\ncat(\"Number of duplicated rows in MC2_nodes:\", num_removed_rows, \"\\n\")\n\n\nNumber of duplicated rows in MC2_nodes: 0 \n\n\nShow the Code\n#Edges\n# Remove duplicated rows and keep only unique rows\nMC2_edges_unique <- distinct(MC2_edges)\n\n# Calculate number of removed duplicated rows\nnum_removed_rows <- nrow(MC2_edges) - nrow(MC2_edges_unique)\n\n# Print the number of duplicated rows\ncat(\"Number of duplicated rows in MC2_edges:\", num_removed_rows)\n\n\nNumber of duplicated rows in MC2_edges: 155291\n\n\nWhile it is possible to have multiple identical shipments between companies, it does seem odd that everything is identical down to the weight of the load. As such, we will assume these rows to be errors in the data and remove them from subsequent analyses.\nA new dataframe with only the unique shipments, will be saved under ‘MC2_edges_unique’.\n\n\n3.4 Reviewing list of HS Codes\nReview the list of hscodes, we note that there are 4,761 unique codes. It is unclear however, if all codes are related to the fishing industry.\n\n\nShow the Code\n# Calculate the number of unique hscode values\nnum_unique_hscode <- MC2_edges_unique %>%\n  distinct(hscode) %>%\n  n_distinct()\n\n# Print the number of unique hscode values\ncat(\"Number of unique hscode values in MC2_edges_unique:\", num_unique_hscode, \"\\n\")\n\n\nNumber of unique hscode values in MC2_edges_unique: 4761 \n\n\nTo further explore this, hscodes were compared with the Singapore Trade Classification, Customs and Excise Duties document. We will look at the top 5 nodes to get a sense of which categories\n\n\n\n\n\n\nNote\n\n\n\nSingapore adopts the 8-digit HS Codes in the ASEAN Harmonised Tariff Nomenclature (AHTN), which is based on the WCO 6-digit level HS Codes. We will be using the first 6 digits to match with codes in our dataset.\n\n\n\n\nShow the Code\ntop_5_hscodes <- MC2_edges_unique %>%\n  count(hscode, sort = TRUE) %>%\n  top_n(5)\n\nprint(top_5_hscodes)\n\n\n# A tibble: 5 × 2\n  hscode      n\n  <chr>   <int>\n1 306170 156204\n2 950300 123262\n3 870899 108353\n4 611020  90772\n5 940360  89764\n\n\nThe table below shows the top 5 HS codes, and its corresponding product categories. This suggest that significant proportion of links on the edges dataset are not related to fishing or related industries.\n\n\n\n\n\n\n\n\nNo.\nHS Code\nCategory\n\n\n\n\n1\n306170\nNo Match\n\n\n2\n950300\nTricycles, scooters, pedal cars and similar wheeled toys; dolls’ carriages; dolls; other toys; reduced-size (“scale”) models and similar recreational models, working or not; puzzles of all kinds:\n\n\n3\n870899\nParts, for motor vehicles\n\n\n4\n611020\nJerseys, pullovers, cardigans, waistcoats and similar articles, knitted or crocheted - of cotton\n\n\n5\n940360\nOther furniture and parts - other wooden furniture\n\n\n\nAs such, using the HS code document as a guide, the following categories were found to be relevant to our area of interest:\n\n\n\n\n\n\n\n\nNo.\nCategories\nCorresponding Codes\n\n\n\n\n1\nChapter 3: Fish and crustaceans, molluscs and other aquatic invertebrates.\n0301, 0302, 0303, 0304, 0305, 0306, 0307, 0308, 0309\n\n\n2\nChapter 15: Animal, vegetable or microbial fats and oils and their cleavage products\n1504\n\n\n3\nChapter 16: Preparations of meat, of fish, crustaceans, molluscs or other aquatic 73 invertebrates, or of insects.\n1603, 1604, 1605\n\n\n4\nChapter 21: Flours, meals and pellets, of meat or meat offal, of fish or of crustaceans, molluscs or other aquatic invertebrates, unfit for human consumption; greaves\n2301\n\n\n\nUsing the stringr() and filter() function, we will extract shipments that match the first 4 digits from the table above.This narrows down the number of relevant links to 2,050,59.\n\n\nShow the Code\ndesired_hscodes <- c(\"0301\", \"0302\", \"0303\", \"0304\", \"0305\", \"0306\", \"0307\", \"0308\", \"0309\", \"1504\", \"1603\", \"1604\", \"1605\", \"2301\")\n\nMC2_edges_fishing <- MC2_edges_unique %>%\n  filter(str_sub(hscode, start = 1, end = 4) %in% desired_hscodes)\n\n\n\n\n3.5 Wrangling attributes\nA close examination of the ’MC2_edges_unique’ dataframe reveals that it consists of individual transactions. This is not very useful for visualisation.\nIn view of this, we will aggregate the rows in two ways. First by source, target and year, and second by source, target and yearmonth. To do so, we will use the following 4 functions from dplyr package; filter(), group(), summarise(), and ungroup().\nThe 2 new dataframes, edges_ym and edges_y, will also include of the following variables:\n\nWeight: Number of shipments between two entities\nsumweightkg = Total weight of products for all shipments\navg_weight_per_shipment: Average weight of products per shipment (sumweightkg/weight)\n\n\n\n\n\n\n\nNote\n\n\n\nAs the remaining variables such as valueofgoods_omu, volumeteu and valueofgoodsusd recorded a high proportion of ‘NA’ or ‘O’, we will exclude them the new dataframe.\n\n\n\n\nShow the Code\nedges_ym <- MC2_edges_fishing %>%\n  group_by(source, target, yearmonth) %>%\n  summarise(weight = n(), sumweightkg = sum(weightkg), .groups = 'drop') %>%\n  filter(source != target) %>%\n  filter(weight > 1) %>%\n  ungroup() %>%\n  mutate(avg_shipmentwt = sumweightkg / weight)\n\n# Convert yearmonth to date format\nedges_ym$yearmonth <- as.Date(paste0(edges_ym$yearmonth, \"-01\"))\n\nedges_y <- MC2_edges_fishing %>%\n  group_by(source, target, year) %>%\n  summarise(weight = n(), sumweightkg = sum(weightkg), .groups = 'drop') %>%\n  filter(source != target) %>%\n  filter(weight > 1) %>%\n  ungroup() %>%\n  mutate(avg_shipmentwt = sumweightkg / weight)\n\n\nUsing glimpse() to review the new dataframe, we see many relationships having less than handful of shipments between each other. Splitting the values up into quartiles, we see that up to 90% of all links have 10 or less shipments between them on a monthly basis.\n\n\nShow the Code\nglimpse(edges_y)\n\n\nRows: 17,306\nColumns: 6\n$ source         <chr> \" Direct Herring Company Transit\", \" Direct Limited Lia…\n$ target         <chr> \"Caracola Azul NV Nautical\", \"bǐ mù yú A/S Shipping\", \"…\n$ year           <dbl> 2032, 2029, 2030, 2029, 2029, 2034, 2034, 2029, 2031, 2…\n$ weight         <int> 4, 2, 3, 2, 6, 2, 3, 2, 2, 12, 21, 16, 54, 28, 2, 4, 2,…\n$ sumweightkg    <int> 46105, 3140, 5080, 40005, 204695, 33360, 74495, 12315, …\n$ avg_shipmentwt <dbl> 11526.250, 1570.000, 1693.333, 20002.500, 34115.833, 16…\n\n\nShow the Code\nglimpse(edges_ym)\n\n\nRows: 37,958\nColumns: 6\n$ source         <chr> \" Direct Herring Company Transit\", \" Direct Herring Com…\n$ target         <chr> \"Caracola Azul NV Nautical\", \"Caracola Azul NV Nautical…\n$ yearmonth      <date> 2032-04-01, 2032-10-01, 2029-12-01, 2030-11-01, 2029-0…\n$ weight         <int> 2, 2, 2, 3, 2, 4, 2, 2, 2, 6, 6, 2, 4, 6, 7, 2, 10, 6, …\n$ sumweightkg    <int> 18365, 27740, 3140, 5080, 40005, 102340, 102355, 12315,…\n$ avg_shipmentwt <dbl> 9182.500, 13870.000, 1570.000, 1693.333, 20002.500, 255…\n\n\nShow the Code\nquartiles_y <- quantile(edges_y$weight, probs = c(0, 0.25, 0.5, 0.75, 0.9, 1))\nprint(quartiles_y)\n\n\n  0%  25%  50%  75%  90% 100% \n   2    2    4    8   22 1152 \n\n\nShow the Code\nquartiles_ym <- quantile(edges_ym$weight, probs = c(0, 0.25, 0.5, 0.75, 0.9, 1))\nprint(quartiles_ym)\n\n\n  0%  25%  50%  75%  90% 100% \n   2    2    2    4   10  195 \n\n\n\n\n\n\n\n\nNote\n\n\n\nAs these will lead to the network being sparse and challenging to analyse, it will be sensible to remove relationships with only a handful of transactions/shipments. Since only 10% of all business transactions between companies process shipments of 10 and above, we will use this as a guide to filter the data.\n\n\nBoth dataseta are complete, with no NA value.\n\n\nShow the Code\nedges_ym_10 <- edges_ym %>%\n  filter(weight >= 10)\n\nedges_y_22 <- edges_y %>%\n  filter(weight >= 22)\n\nedges_ym_10 %>%\n  summarise_all(~ sum(is.na(.)) / n())\n\n\n# A tibble: 1 × 6\n  source target yearmonth weight sumweightkg avg_shipmentwt\n   <dbl>  <dbl>     <dbl>  <dbl>       <dbl>          <dbl>\n1      0      0         0      0           0              0\n\n\nShow the Code\nedges_y_22 %>%\n  summarise_all(~ sum(is.na(.)) / n())\n\n\n# A tibble: 1 × 6\n  source target  year weight sumweightkg avg_shipmentwt\n   <dbl>  <dbl> <dbl>  <dbl>       <dbl>          <dbl>\n1      0      0     0      0           0              0\n\n\n\n\n3.6 Zooming into outliers\nLooking at the average weight of shipment across the 6 years was quite interesting. Specifically, we see a small proportion of shipments that are transacting way beyond the ‘normal’ weights, with the heaviest going up to 307,800kg. We will filter out companies involved in these transactions and all activities there are associated with to further focus subsequent analyses.\n\n\nShow the Code\np3 <- ggplot(data = edges_ym_10, aes(x = avg_shipmentwt)) +\n  geom_histogram(binwidth = 100, fill = \"steelblue\", color = \"black\") +\n  labs(x = \"Average Weight per Shipment\", y = \"Frequency\", title = \"Distribution of Average Weight per Shipment\") +\n  theme_classic()\n\np3 <- ggplotly(p3, tooltip = c(\"avg_shipmentwt\"))\np3\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that after filtering for only outlier nodes, the proportion of nodes with NA for ‘shpcountry’ and ‘rcvcountry’ has decreased significantly. We will replace NA with Unknown.\n\n\n\n\nShow the Code\n# Calculate the IQR and determine the outlier threshold\nQ1 <- quantile(edges_ym_10$avg_shipmentwt, 0.25)\nQ3 <- quantile(edges_ym_10$avg_shipmentwt, 0.75)\nIQR <- Q3 - Q1\noutlier_threshold <- 1.5 * IQR\n\n# Identify outliers\noutliers <- edges_ym_10$avg_shipmentwt < (Q1 - outlier_threshold) | edges_ym_10$avg_shipmentwt > (Q3 + outlier_threshold)\n\n# Add a new column to mark outliers\nedges_ym_10$outlier <- ifelse(outliers, \"Yes\", \"No\")\n\n# Filter source companies involved in outlier transactions\nfiltered_df <- edges_ym_10 %>%\n  group_by(source) %>%\n  filter(\"Yes\" %in% outlier) %>%\n  ungroup()\n\nedges_ym_outlier <- filtered_df %>%\n  mutate(year = substr(yearmonth, 1, 4))\n\n##Nodes for outliers\n# Get unique values from 'source' and 'target' columns\nunique_outliers <- unique(c(edges_ym_outlier$source, edges_ym_outlier$target))\n\n# Filter 'MC2_nodes' based on unique values\nnodes_ym_outlier<- MC2_nodes %>%\n  filter(id %in% unique_outliers)\n\n# Check for NA\nnodes_ym_outlier %>%\n  summarise_all(~ sum(is.na(.)) / n())\n\n\n# A tibble: 1 × 3\n     id shpcountry rcvcountry\n  <dbl>      <dbl>      <dbl>\n1     0     0.0619     0.0567\n\n\nShow the Code\n# Replace with unknown\nnodes_ym_outlier <- nodes_ym_outlier %>%\n  mutate(\n    shpcountry = if_else(is.na(shpcountry), \"Unknown\", shpcountry),\n    rcvcountry = if_else(is.na(rcvcountry), \"Unknown\", rcvcountry)\n  )\n\n##Nodes for ym\n# Step 1: Get unique values from 'source' and 'target' columns\nunique_values <- unique(c(edges_ym_10$source, edges_ym_10$target))\n\n# Step 2: Filter 'MC2_nodes' based on unique values\nnodes_ym_10<- MC2_nodes %>%\n  filter(id %in% unique_values)\n\n##Nodes for y\n# Step 1: Get unique values from 'source' and 'target' columns\nunique_values <- unique(c(edges_y_22$source, edges_y_22$target))\n\n# Step 2: Filter 'MC2_nodes' based on unique values\nnodes_y_22<- MC2_nodes %>%\n  filter(id %in% unique_values)"
  },
  {
    "objectID": "Take-Home_Exercises/Take-Home_Ex02/take-home_Ex02.html#understanding-overall-networks",
    "href": "Take-Home_Exercises/Take-Home_Ex02/take-home_Ex02.html#understanding-overall-networks",
    "title": "Take Home Exercise 2",
    "section": "4 Understanding overall networks",
    "text": "4 Understanding overall networks\n\n\nShow the Code\nMC2_graph <- tbl_graph(nodes = MC2_nodes_final,\n                           edges = MC2_edges_filtered5, \n                           directed = TRUE)\n\nMC2_graph\n\n\n# A tbl_graph: 2213 nodes and 4461 edges\n#\n# A directed acyclic simple graph with 83 components\n#\n# A tibble: 2,213 × 3\n  id                                shpcountry rcvcountry\n  <chr>                             <chr>      <chr>     \n1 Yu gan  Sea spray GmbH Industrial Oceanus    Oceanus   \n2 Olas del Mar Worldwide            Oceanus    Oceanus   \n3 Panope Limited Liability Company  Vesperanda Oceanus   \n4 hǎi dǎn Corporation Wharf         Marebak    Oceanus   \n5 Sea Breezes GmbH & Co. KG Shark   Oceanus    Oceanus   \n6 Costa de la Felicidad Shipping    Alverossia Oceanus   \n# ℹ 2,207 more rows\n#\n# A tibble: 4,461 × 5\n   from    to Weight sumweightkg avg_weight_per_shipment\n  <int> <int>  <int>       <int>                   <dbl>\n1   672    59      6        8380                   1397.\n2   667   383      6      204695                  34116.\n3  2015    19    131     1116280                   8521.\n# ℹ 4,458 more rows\n\n\n\n\nShow the Code\nMC2_graph %>%\n  activate(edges) %>%\n  arrange(desc(Weight))\n\n\n# A tbl_graph: 2213 nodes and 4461 edges\n#\n# A directed acyclic simple graph with 83 components\n#\n# A tibble: 4,461 × 5\n   from    to Weight sumweightkg avg_weight_per_shipment\n  <int> <int>  <int>       <int>                   <dbl>\n1  1109  1110   6309  1102289340                 174717.\n2   352    16   3375   147406605                  43676.\n3   223     3   1970   117350260                  59569.\n4   120    60   1420    27804875                  19581.\n5   420     4   1351    17991400                  13317.\n6   223     5   1312    62279680                  47469.\n# ℹ 4,455 more rows\n#\n# A tibble: 2,213 × 3\n  id                                shpcountry rcvcountry\n  <chr>                             <chr>      <chr>     \n1 Yu gan  Sea spray GmbH Industrial Oceanus    Oceanus   \n2 Olas del Mar Worldwide            Oceanus    Oceanus   \n3 Panope Limited Liability Company  Vesperanda Oceanus   \n# ℹ 2,210 more rows\n\n\n\n\nShow the Code\nggraph(MC2_graph,\n       layout = \"nicely\") +\n  geom_edge_link(aes(width = Weight),\n                 alpha = 0.2) +\n  geom_node_point()\n\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead."
  },
  {
    "objectID": "Take-Home_Exercises/Take-Home_Ex02/take-home_Ex02.html#understanding-the-data",
    "href": "Take-Home_Exercises/Take-Home_Ex02/take-home_Ex02.html#understanding-the-data",
    "title": "Take Home Exercise 2",
    "section": "4 Understanding the data",
    "text": "4 Understanding the data\n\n4.1 Nodes\nBefore plotting the networks. Let’s look at each data set seperately. Using glimspe(), we see that post data cleaning, there are 194 unique nodes involved in outlier transactions in the dataframe aggregated by year month.\n\n\nShow the Code\nglimpse(nodes_ym_outlier)\n\n\nRows: 194\nColumns: 3\n$ id         <chr> \"Olas del Mar Worldwide\", \"Panope Limited Liability Company…\n$ shpcountry <chr> \"Oceanus\", \"Vesperanda\", \"Marebak\", \"Oceanus\", \"Alverossia\"…\n$ rcvcountry <chr> \"Oceanus\", \"Oceanus\", \"Oceanus\", \"Oceanus\", \"Oceanus\", \"Oce…\n\n\nNext, we use the count() function to determine the number of companies that are mostly associated to a specific shipping country. This may provide a sense on where a company may have originated from. Dataframe nodes_y_22 will be used for this analyses since there are more nodes relative to nodes_ym_10. A quick view of the data indicates that these companies are associated with 45 different countries.\n\n\nShow the Code\nsummary_shpcountry <- nodes_ym_outlier %>% count(shpcountry)\n\nsummary_shpcountry <- summary_shpcountry %>%\n  group_by(shpcountry) %>%\n  summarise(count = sum(n))\n\nsummary_shpcountry\n\n\n# A tibble: 45 × 2\n   shpcountry   count\n   <chr>        <int>\n 1 -22004           1\n 2 Alverossia       6\n 3 Arreciviento     8\n 4 Arvekalia        1\n 5 Brindivaria      3\n 6 Coral Solis      3\n 7 Coralada         4\n 8 Coralmarica      6\n 9 Faraluna         4\n10 Gavanovia        2\n# ℹ 35 more rows\n\n\nA bar chart is then plotted for the top five countries among all source firms, and outlier source firms i.e., countries with the most number of ‘outlier’ shipping firms being associated with. Rows with ‘shpcountry’ as ‘Unknown’ were excluded from this analyses.\n\n\n\n\n\n\nNote\n\n\n\nLooking at the overall chart in orange, considering that there are a total of 59 unique countries, it is interesting to note that the top 5 listed in the bar chart below were associated with approximately half of all companies. These countries are likely the bigger players in the fishing industry.\nNotably, some changes to the ranking are observed among firms involved in outlier transactions. Oceanus moves to the top spot, with Mawazam at second. Marebak moves from first to third.\n\n\n\n\nShow the Code\nsummary_shpcountry <- nodes_y_22 %>%\n  filter(shpcountry != \"Unknown\") %>%\n  count(shpcountry)\n\ntop_5_shpcountry <- summary_shpcountry %>%\n  top_n(5, n)\n\ntotal_count <- sum(summary_shpcountry$n)\n\np1 <- ggplot(top_5_shpcountry, aes(x = reorder(shpcountry, -n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"orange\") +\n  labs(title = \"Top 5 'shpcountry' by Count\", x = \"\", y = \"Count\") +\n  geom_text(aes(label = paste0(n, \" (\", round((n / total_count) * 100), \"%)\")), vjust = -0.5, color = \"black\") +\n  theme_classic() +\n  coord_cartesian(ylim = c(0, max(top_5_shpcountry$n) * 1.1))\n\nsummary_shpcountry_outlier <- nodes_ym_outlier %>%\n  filter(shpcountry != \"Unknown\") %>%\n  count(shpcountry)\n\ntop_5_countryoutlier <- summary_shpcountry_outlier %>%\n  top_n(5, n)\n\ntotal_count <- sum(summary_shpcountry_outlier$n)\n\np2 <- ggplot(top_5_countryoutlier, aes(x = reorder(shpcountry, -n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue\") +\n  labs(title = \"Top 5 'shpcountry' by Count (Among outliers)\", x = \"Most Associated Shipping Country\", y = \"Count\") +\n  geom_text(aes(label = paste0(n, \" (\", round((n / total_count) * 100), \"%)\")), vjust = -0.5, color = \"black\") +\n  theme_classic() +\n  coord_cartesian(ylim = c(0, max(top_5_countryoutlier$n) * 1.1))\n\n# Combine p1 and p2 plots vertically\ncombined_plot <- plot_grid(p1, p2, ncol = 1, align = \"v\")\n\n# Display the combined plot\ncombined_plot\n\n\n\n\n\n\n\n4.2 Edges\nNext, using the groupby() and summarize() function, we will review the shipment trends across the time period of the provided data.\n\n\nShow the Code\nsummary_yearmonth <- edges_ym_outlier %>%\n  group_by(yearmonth) %>%\n  summarize(total_weight = sum(sumweightkg), total_count = sum(weight)) %>%\n  ungroup() %>%\n  mutate(avg_shipmentwt = total_weight / total_count)\n\n# Check the updated summary_yearmonth data\nprint(summary_yearmonth)\n\n\n# A tibble: 84 × 4\n   yearmonth  total_weight total_count avg_shipmentwt\n   <date>            <int>       <int>          <dbl>\n 1 2028-01-01     27266185         532         51252.\n 2 2028-02-01     30711115         503         61056.\n 3 2028-03-01     35236780         653         53961.\n 4 2028-04-01     26890370         535         50262.\n 5 2028-05-01     36245285         699         51853.\n 6 2028-06-01     37529860         724         51837.\n 7 2028-07-01     35730130         579         61710.\n 8 2028-08-01     38647305         696         55528.\n 9 2028-09-01     32434580         629         51565.\n10 2028-10-01     36289635         620         58532.\n# ℹ 74 more rows\n\n\nLooking at the line charts below, we see that shipment and weight trends largely coincide with one another. However, if we look closer at 2032 where number of shipments generally lulled, there was a significant spike in the total weight of shipments.\n\n\n\n\n\n\nImportant\n\n\n\nThe odd spike in 2032 may point towards possible illegal activities and is worth looking into in subsequent analyses.\n\n\n\n\nShow the Code\np3 <- ggplot(summary_yearmonth, aes(x = yearmonth)) +\n  geom_line(aes(y = total_count, color = \"Total Count\", group = 1), size = 0.5) +\n  labs(x = \"Year\",\n       y = \"Number of Shipments\") +\n  scale_color_manual(values = c(\"Total Count\" = \"steelblue\")) +\n  theme_classic() +\n  theme(legend.position = \"bottom\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\")\n\np4 <- ggplot(summary_yearmonth, aes(x = yearmonth)) +\n  geom_line(aes(y = total_weight, color = \"Total Weight\", group = 1), size = 0.5) +\n  labs(x = \"Year\",\n       y = \"Weight of Shipments\") +\n  scale_color_manual(values = c(\"Total Weight\" = \"orange\")) +\n  theme_classic() +\n  theme(legend.position = \"bottom\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\")\n\n# Convert individual ggplot charts to plotly objects\np3 <- ggplotly(p3, tooltip = c(\"x\", \"y\"))\np4 <- ggplotly(p4, tooltip = c(\"x\", \"y\"))\n\n# Create the subplot\nsubplot <- subplot(p3, p4, nrows = 2, heights = c(0.5, 0.5))\n\n# Remove the legend title\nsubplot <- subplot %>% layout(legend = list(title = list(text = \"\")))\n\n# Set the subplot title\nsubplot <- subplot %>% layout(title = \"Total Number and Weight of Shipments Over Time\",\n                              showlegend = TRUE)\n\n# Adjust the plot margins\nsubplot <- subplot %>% layout(margin = list(t = 50))\n\n# Display the stacked plot with title\nsubplot\n\n\n\n\n\n\nNext, we look at shipment trends by source and target companies over time. One of the telltale signs of illegal fishing is the opening and closing of such companies within short span of time, either because they were caught, or to avoid getting caught.\n\n\n\n\n\n\nNote\n\n\n\nFrom the heatmap below, we can identify a few potential source companies to look into. For example:\n\nShipping S.A. de C.V.: This source node only made 1 transaction over the 6 years; but it’s load was the heaviest.\nSaltsea & Inc Carriers: The shipments by this firm is sporadic, with large time breaks in between.\nMarine Mates Pic Delivery: The shipments by this firm is sporadic.\n\nIt will be interesting to see how these source nodes map out in the overall network.\n\n\n\n\nShow the Code\n# Define custom color palette from blue to yellow\nmy_palette <- c(\"#0000FF\", \"#FFFF00\", \"#FF0000\")  # Example colors\n\n# Plot heatmap with custom color palette\nggplot(edges_ym_outlier, aes(x = yearmonth, y = source, fill = avg_shipmentwt)) +\n  geom_tile() +\n  labs(x = \"Time Period\", y = \"Source\", fill = \"Av. Weight\") +\n  scale_fill_gradientn(colors = my_palette, limits = range(edges_ym_outlier$avg_shipmentwt),\n                       breaks = seq(min(edges_ym_outlier$avg_shipmentwt), max(edges_ym_outlier$avg_shipmentwt), length.out = length(my_palette)),\n                       labels = scales::number_format(accuracy = 0.01),\n                       guide = guide_colorbar(title.position = \"top\")) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 0, hjust = 1),\n        axis.text.y = element_text(size = 3),\n        plot.margin = margin(20, 20, 20, 120),\n        legend.position = \"bottom\",\n        legend.key.height = unit(0.3, \"cm\"),\n        legend.key.width = unit(1.5,\"cm\")) +\n  ggtitle(\"Average weight of Shipment by Source\")"
  },
  {
    "objectID": "Take-Home_Exercises/Take-Home_Ex02/take-home_Ex02.html#understanding-underlying-networks",
    "href": "Take-Home_Exercises/Take-Home_Ex02/take-home_Ex02.html#understanding-underlying-networks",
    "title": "Take Home Exercise 2",
    "section": "5 Understanding underlying networks",
    "text": "5 Understanding underlying networks\n\n5.1 Overall network by in and out degree\nNext, the nodes and edge datasets are plotting into a network graph using visNetwork. Looking at the overall graph, while there is a clear connected graph in the middle, there are also many isolated nodes. The main network however, is clumped together, and it is not very useful visually for analyses.\n\n\nShow the Code\nedges_ym_outlier_g <- edges_ym_outlier %>%\n  rename(from = source, to = target)\n\noutlier_graph <- tbl_graph(nodes = nodes_ym_outlier,\n                           edges = edges_ym_outlier_g, \n                           directed = TRUE)\n\n#Plot outlier graph\nggraph(outlier_graph, layout = 'fr') + \n  geom_edge_link(aes()) +\n  geom_node_point(aes()) +\n  theme_graph() +\n  ggtitle(\"Outlier Graph\")\n\n\n\n\n\nAs such using functions available in tidyverse, we calculate the in and out degree using centrality_degree(), to provide an additional layer.\n\n\nShow the Code\n# Calculate in-degree values\noutlier_graph <- outlier_graph %>%\n  activate(nodes) %>%\n  mutate(in_degree = centrality_degree(mode = \"in\"))\n\n# Plot the graph by in degree\ng <- ggraph(outlier_graph, layout = 'linear', circular = TRUE) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_edge_link(aes(width = weight), color = \"grey\", alpha = 0.2) +\n  geom_node_point(aes(size = in_degree), color = \"lightblue\", alpha = 0.6) +\n  scale_size(range = c(1, 10)) +\n  theme_graph() +\n  ggtitle(\"Outlier nodes by in-degree\")\n\ng\n\n\n\n\n\n\n\nShow the Code\n# Calculate in-degree values\noutlier_graph <- outlier_graph %>%\n  activate(nodes) %>%\n  mutate(out_degree = centrality_degree(mode = \"out\"))\n\n# Plot the graph\ng <- ggraph(outlier_graph, layout = 'linear', circular = TRUE) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_edge_link(aes(width = weight), color = \"grey\", alpha = 0.2) +\n  geom_node_point(aes(size = out_degree), color = \"orange\", alpha = 0.6) +\n  scale_size(range = c(1, 10)) +\n  theme_graph() +\n  ggtitle(\"Outlier nodes by out-degree\")\n\ng\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFrom the charts, we can better understand the proportion of key players with a high number of in and out degrees.Specifically:\n\nWe see a smaller proportion of nodes with high in-degrees, approximately 20% of all nodes.\nThis trend is slightly different when we look at out-degree, as we observe a larger proportion of bigger nodes in the circular graph below.\n\nThose with high in-degree represents companies that take in a lot of shipments, while those with high out-degress are those that delivery many shipments. These nodes are likely key players in the eco-system and act like hubs. However, a large majority of nodes are connected to only a handful of other nodes. This trend was observed in both charts.\nKeep in mind that this dataset is limited to companies with at least one transaction that is considered an outlier i.e., in larger amounts. While is may not be surprising for ‘hub’ nodes to transact in large amounts, it comes across fishy when small players with few partners are transacting at such large amounts, and to a small number of business partners.\n\n\n\n\n5.2 Zooming into nodes of interest\nRecall earlier that we identifed a few companies with odd transaction patterns over the years.\n\nShipping S.A. de C.V.: This source node only made 1 transaction over the 6 years; but it’s load was the heaviest.\nSaltsea & Inc Carriers: The shipments by this firm is sporadic, with large time breaks in between.\nMarine Mates Pic Delivery: The shipments by this firm is sporadic.\n\nFor the scope of this exercise, let’s explore Saltsea & Inc Carriers. Clicking onto the node in the graph, we see the company only has 1 partner, Pao Gan SE Seal. Since this is the only edge on the graph tied to Saltsea & Inc, we will take a closer look at it’s partners to understand business patterns.\n\n\nShow the Code\n# Rename 'source' column to 'from' and 'target' column to 'to' \nedges_ym_outlier_g <- edges_ym_outlier %>%\n  rename(from = source, to = target)\n\nlibrary(visNetwork)\n\n# Create the network graph\nvisNetwork(\n  nodes = nodes_ym_outlier,\n  edges = edges_ym_outlier_g\n) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %>%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\"))\n\n\n\n\n\n\n\n\n5.3 Transaction Patterns - Pao Gan SE Seal and Mar del Este CJSC\nExtracting only business partners of Pao Gan SE Seal, we see that the company has multiple partners with long intervals between shipments. To assess if this is common within the industry, we turn to Saltsea & Inc Carriers’ second partner Mar del Este CJSC. It is interesting to note that transaction patterns from both companies reveal similar business patterns i.e., a mix of long term, and short term partners.\n\n\nShow the Code\nPao_df <- edges_ym_outlier[edges_ym_outlier$target == \"Pao gan SE Seal\", ]\n\nggplot(Pao_df, aes(x = yearmonth, y = source, fill = avg_shipmentwt)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"lightyellow\", high = \"red\") +\n  labs(x = \"Year-Month\", y = \"Source Company\", fill = \"Av. weight\") +\n  ggtitle(\"Shipment patterns by Av. Weight - Pao gan SE Seal\")\n\n\n\n\n\n\n\nShow the Code\nMar_df <- edges_ym_outlier[edges_ym_outlier$target == \"Mar del Este CJSC\", ]\n\nggplot(Mar_df, aes(x = yearmonth, y = source, fill = avg_shipmentwt)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"lightyellow\", high = \"red\") +\n  labs(x = \"Year-Month\", y = \"Source Company\", fill = \"Av. weight\") +\n  ggtitle(\"Shipment patterns by Av. Weight - Mar del Este CJSC\")\n\n\n\n\n\n\n\n5.4 Proportion of outlier transactions - Pao Gan SE Seal\nPerhaps a additional layer to look at, would be the proportion of ‘outlier’ transactions made by these source nodes.\n\n\n\n\n\n\nNote\n\n\n\nFrom the table below, we can see that for the business partners of Pao Gan SE Seal, there were 3 companies where all shipments were heavy and considered outliers. These companies are:\n\nSaltSea & Inc Carriers\nNiger River Delta Oyj Abalone, and\nScottish Oysters Flotsam S.p.A. Services.\n\nWhile inconclusive, the trick to catching illegal fishing activities may indeed involve multiple variables.\n\n\n\n\nShow the Code\n# Calculate the total count, proportion of \"Yes\" outliers, and proportion of \"No\" outliers based on the source\nproportion_data <- Pao_df %>%\n  group_by(source) %>%\n  summarise(\n    total_count = n(),\n    proportion_yes = sum(outlier == \"Yes\") / n(),\n    proportion_no = sum(outlier == \"No\") / n()\n  )\n\nlibrary(knitr)\nlibrary(kableExtra)\n\n# Convert proportion_data table to a nice table format\nproportion_table <- kable(proportion_data, format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n# Display the table\nproportion_table\n\n\n\n\n \n  \n    source \n    total_count \n    proportion_yes \n    proportion_no \n  \n \n\n  \n    Adriatic Tuna Seabass BV Transit \n    4 \n    0.0 \n    1.0 \n  \n  \n    Costa de Coral SRL United \n    7 \n    0.0 \n    1.0 \n  \n  \n    Neptune's Harvest A/S Hijiki \n    5 \n    0.6 \n    0.4 \n  \n  \n    Niger River Delta  Oyj Abalone \n    7 \n    1.0 \n    0.0 \n  \n  \n    SaltSea & Inc Carriers \n    16 \n    1.0 \n    0.0 \n  \n  \n    Scottish Oysters Flotsam S.p.A. Services \n    10 \n    1.0 \n    0.0 \n  \n  \n    Shou gan  Oyj Overseas \n    1 \n    0.0 \n    1.0"
  },
  {
    "objectID": "Take-Home_Exercises/Take-Home_Ex02/take-home_Ex02.html#conclusion-and-suggestions-for-group-project",
    "href": "Take-Home_Exercises/Take-Home_Ex02/take-home_Ex02.html#conclusion-and-suggestions-for-group-project",
    "title": "Take Home Exercise 2",
    "section": "6 Conclusion and Suggestions for Group Project",
    "text": "6 Conclusion and Suggestions for Group Project\n\nLooking at the trade dataset, a sizable proportion of businesses trade exceptionally high weight of products. It is crucial that we look at these data points to better understand such occurrences.\nA smaller proportion of nodes with high in-degrees were noted, meaning aside from a few, businesses receiving goods tend to not have as many transactions, relative to those shipping produce. This is in line with findings from our heatmap, as we see many companies shipping throughout the year, across the 6 years.\nHowever, it is also common for businesses to pause and have long intervals between shipments. While this may come across as suspicious, it is likely insufficient to signal illegal activities. As such, it may be useful to explore other indicators as well, a view them as a group. In this exercise, we studied two indicators transaction patterns and proportion of outlier shipments. Other potential indicators may be:\n\nNumber of small one-off shipments\nNumber of business partners - assuming most would not want to be associated with such activities"
  },
  {
    "objectID": "Take-Home_Exercises/Take-Home_Ex02/take-home_Ex02_v2.html",
    "href": "Take-Home_Exercises/Take-Home_Ex02/take-home_Ex02_v2.html",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "Seafood is one of the most widely traded commodities in the global food market. More than a third of the world’s population relies on fish and other seafood as a primary source of protein in their diet, and an estimated 520 million people make their livelihoods through fishing or fishing-related activities.\nUnfortunately, illegal, unreported, and unregulated fishing is a major contributor to over fishing worldwide. These activities pose a threat not only to fragile marine ecosystems, but also to food security in coastal communities and regional stability more broadly. The illegal fishing trade has been linked to organized crime, and human rights violations are common when fishing operations are conducted without regulatory oversight.\nThe country of Oceanus has sought FishEye International’s help in identifying companies possibly engaged in illegal, unreported, and unregulated (IUU) fishing. Using visual analytics, this challenge will sought to help FishEye identify companies that may be engaged in illegal fishing.\n\n\nThe data used for this exercises will be from Mini Challenge 2 of the VAST Challenge. This includes 2 main files; the first dataset in json format consists of 34,552 nodes and 5,464,092 directed edges, while the second set of files, a bundle of 12, each represent the output of an AI program for link reference Each bundle represents a list of potential edges to add to the main graph. Details of the attributes provided are listed below:\nNode:\n\nid: Name of the company that originated (or received) the shipment\nshpcountry: Country the company most often associated with when shipping\nrcvcountry: Country the company most often associated with when receiving\ndataset: Always ‘MC2’\n\nEdge:\n\narrivaldate: Date the shipment arrived at port in YYYY-MM-DD format.\nhscode: Harmonized System code for the shipment. Can be joined with the hscodes table to get additional details.\nvalueofgoods_omu: Customs-declared value of the total shipment, in Oceanus Monetary Units (OMU)\nvolumeteu: The volume of the shipment in ‘Twenty-foot equivalent units’, roughly how many 20-foot standard containers would be required. (Actual number of containers may have been different as there are 20ft and 40ft standard containers and tankers that do not use containers)\nweightkg: The weight of the shipment in kilograms (if known)\ndataset: Always ‘MC2’\ntype: Always ‘shipment’ for MC2\ngenerated_by: Name of the program that generated the edge. (Only found on ‘bundle’ records.)\n\n\n\n\n\n\n\nNote\n\n\n\nFor the purpose of this exercise, only the main json file, excluding the bundle files, would be used.\n\n\n\n\n\nThe main objective of this exercise is to use visual analytics to identify temporal patterns for individual entities and between entities in the knowledge graph FishEye created from trade records, and categorize the types of business relationship patterns."
  },
  {
    "objectID": "Take-Home_Exercises/Take-Home_Ex02/take-home_Ex02_v2.html#load-and-import-data",
    "href": "Take-Home_Exercises/Take-Home_Ex02/take-home_Ex02_v2.html#load-and-import-data",
    "title": "Take Home Exercise 2",
    "section": "2 Load and Import Data",
    "text": "2 Load and Import Data\n\n2.1 Load R Packages\nFirst, the necessary packages are installed and loaded onto the RStudio environment.\n\n\nShow the Code\npacman::p_load(jsonlite, tidygraph, ggraph, visNetwork, tidyverse, igraph, ggraph, lubridate, clock, graphlayouts, viridis, ggdist, colorspace, ggdist, ggthemes, colorspace, scales, cowplot, visNetwork, plotly, stringr, patchwork, viridis, ggstatsplot, kableExtra)\n\n\n\n\n2.2 Importing the data\nUnlike earlier exercises where the data files were in csv format, the data files provided in the challenge are in json format. As such, the dataset will he imported using jsonlite::fromJSON and saved as ‘MC2’.\n\n\nShow the Code\nMC2 <- jsonlite::fromJSON(\"/Users/jiahuiloh/lohjiahui/ISSS608-VAA/Take-Home_Exercises/Take-Home_Ex02/data/mc2_challenge_graph.json\")\n\n\nNext we will convert the file into tibble format using as_tibble. The ‘nodes’ list will be saved as’MC2_nodes’, and ’links’ list as ’MC2_edges’. The variables in the lists will also be re-ordered to facilitate subsequent visualization tasks. The details are as follows:\nMC2_nodes: id, shpcountry, rcvcountry\nMC2_edges: source, target,arrivaldate, hscode, weightkg, valueofgoods_omu, valueofgoodsusd, volumeteu\nFor both, ‘dataset’ column is dropped since it does not provide any additional information or insights.\n\n\nShow the Code\nMC2_nodes <- as_tibble(MC2$nodes) %>%\n  select(id,shpcountry,rcvcountry) \n\nMC2_edges <- as_tibble(MC2$links) %>%\n  select(source, target, arrivaldate, hscode, weightkg, valueofgoods_omu, valueofgoodsusd, volumeteu)"
  },
  {
    "objectID": "Take-Home_Exercises/Take-Home_Ex02/take-home_Ex02_v2.html#data-review-and-wrangling",
    "href": "Take-Home_Exercises/Take-Home_Ex02/take-home_Ex02_v2.html#data-review-and-wrangling",
    "title": "Take Home Exercise 2",
    "section": "3 Data Review and Wrangling",
    "text": "3 Data Review and Wrangling\nAfter a quick exploration of the data, Looking a few problems were observed:\n\nVariable ‘arrivaldate’ in <chr> format, should be transformed to <date> format\nLarge proportions of missing data, recorded as ‘NA’ or ‘0’ under valueofgoods_omu, valueofgoodsusd and volumeteu.\nMultiple duplicated rows in the edges list\nIrrelevant hscodes i.e., not associated with the fishing industry\n\n\n3.1 Correcting for ‘date’ format\nThe following subsections will document the cleaning process.\nUsing the glimpse() function from the dplyr library, we review the columns in the new dataframes.\n\n\nShow the Code\nglimpse(MC2_nodes)\n\n\nRows: 34,576\nColumns: 3\n$ id         <chr> \"AquaDelight Inc and Son's\", \"BaringoAmerica Marine Ges.m.b…\n$ shpcountry <chr> \"Polarinda\", NA, \"Oceanus\", NA, \"Oceanus\", \"Kondanovia\", NA…\n$ rcvcountry <chr> \"Oceanus\", NA, \"Oceanus\", NA, \"Oceanus\", \"Utoporiana\", NA, …\n\n\nShow the Code\nglimpse(MC2_edges)\n\n\nRows: 5,464,378\nColumns: 8\n$ source           <chr> \"AquaDelight Inc and Son's\", \"AquaDelight Inc and Son…\n$ target           <chr> \"BaringoAmerica Marine Ges.m.b.H.\", \"BaringoAmerica M…\n$ arrivaldate      <chr> \"2034-02-12\", \"2034-03-13\", \"2028-02-07\", \"2028-02-23…\n$ hscode           <chr> \"630630\", \"630630\", \"470710\", \"470710\", \"470710\", \"47…\n$ weightkg         <int> 4780, 6125, 10855, 11250, 11165, 11290, 9000, 19490, …\n$ valueofgoods_omu <dbl> 141015, 141015, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ valueofgoodsusd  <dbl> NA, NA, NA, NA, NA, NA, 87110, 188140, NA, 221110, 58…\n$ volumeteu        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe output report of ‘MC2_edges’ above reveals that the ‘arrivaldate’ is treated as ‘Character’ data type instead of date data type. This is an error! Before we continue, it is important for us to change the data type of ‘arrivaldate’ field back to ‘Date’ data type.\n\n\nTo correct the data type for ‘arrivaldate’ we will use the as.Date() function of lubridate package. Additionally, we will create 3 new columns to extract the year month, year and month data from ‘arrivaldate’. Doing so will allow us to look at broader and seasonal patterns, if any, in subsequent analyses.\n\n\nShow the Code\nMC2_edges <- MC2_edges %>%\n  mutate(arrivaldate = as.Date(arrivaldate),\n         yearmonth = format(arrivaldate, \"%Y-%m\"),\n         year = year(arrivaldate),\n         month = factor(month.abb[month(arrivaldate)], levels = month.abb, ordered = TRUE))\n\n# Convert yearmonth to date format\nMC2_edges$yearmonth <- as.Date(paste0(MC2_edges$yearmonth, \"-01\"))\n\n\n\n\n3.2 Completeness of data provided\nTo assess which variables may be most useful for analysis, we use the summarise_all() function to review each variable to understand how complete the data provided is.\nMC2_node:\n\nThere were 0 missing values in the ID column.\nMore than half of the rows in ‘shpcountry’ were recorded as NA. Should we decide to use this data column, it should be used with caution.\n\n\n\nShow the Code\nMC2_nodes %>%\n  summarise_all(~ sum(is.na(.)) / n())\n\n\n# A tibble: 1 × 3\n     id shpcountry rcvcountry\n  <dbl>      <dbl>      <dbl>\n1     0      0.647     0.0841\n\n\nMC2_edges:\n\nLooking at the two variables indicating value of goods in both omu and usd, a high proportion of NA was recorded.\nApproximately 84% of records in the ‘volumeteu’ column were recorded as 0, and may not be very useful for analyses.\nNo missing values, however, was noted for the weightkg column. It may therefore be more efficient to use this column for analyses.\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen eyeballing the data, the ‘volumeteu’ column appeared to have a lot of ‘0’ value. As such, to calculate the proportion of ‘0’, we use the summarise() and mean() function.\n\n\n\n\nShow the Code\nMC2_edges %>%\n  summarise_all(~ sum(is.na(.)) / n())\n\n\n# A tibble: 1 × 11\n  source target arrivaldate hscode weightkg valueofgoods_omu valueofgoodsusd\n   <dbl>  <dbl>       <dbl>  <dbl>    <dbl>            <dbl>           <dbl>\n1      0      0           0      0        0             1.00           0.552\n# ℹ 4 more variables: volumeteu <dbl>, yearmonth <dbl>, year <dbl>, month <dbl>\n\n\nShow the Code\nMC2_edges %>%\n  summarise(zero_proportion = mean(volumeteu == 0, na.rm = TRUE))\n\n\n# A tibble: 1 × 1\n  zero_proportion\n            <dbl>\n1           0.842\n\n\n\n\n\n\n\n\nNote\n\n\n\nWith this summary in mind, subsequent analyses would focus on using the weight of shipments to understand trade relationships between companies.\n\n\n\n\n3.3 Checking for duplicates\nAs both datasets are relatively large. We will also check for duplicates in both dataframes using the distinct() function from dplyr.\nMC2_nodes: no duplicated rows were found.\nMC2_edges: 155,291 duplicated rows were identified\n\n\nShow the Code\n#Nodes\n# Remove duplicated rows and keep only unique rows\nMC2_nodes_unique <- distinct(MC2_nodes)\n\n# Calculate number of removed duplicated rows\nnum_removed_rows <- nrow(MC2_nodes) - nrow(MC2_nodes_unique)\n\n# Print the number of duplicated rows\ncat(\"Number of duplicated rows in MC2_nodes:\", num_removed_rows, \"\\n\")\n\n\nNumber of duplicated rows in MC2_nodes: 0 \n\n\nShow the Code\n#Edges\n# Remove duplicated rows and keep only unique rows\nMC2_edges_unique <- distinct(MC2_edges)\n\n# Calculate number of removed duplicated rows\nnum_removed_rows <- nrow(MC2_edges) - nrow(MC2_edges_unique)\n\n# Print the number of duplicated rows\ncat(\"Number of duplicated rows in MC2_edges:\", num_removed_rows)\n\n\nNumber of duplicated rows in MC2_edges: 155291\n\n\nWhile it is possible to have multiple identical shipments between companies, it does seem odd that everything is identical down to the weight of the load. As such, we will assume these rows to be errors in the data and remove them from subsequent analyses.\nA new dataframe with only the unique shipments, will be saved under ‘MC2_edges_unique’.\n\n\n3.4 Reviewing list of HS Codes\nReviewing the list of hscodes, we note that there are 4,761 unique codes. It is unclear however, if all codes are related to the fishing industry.\n\n\nShow the Code\n# Calculate the number of unique hscode values\nnum_unique_hscode <- MC2_edges_unique %>%\n  distinct(hscode) %>%\n  n_distinct()\n\n# Print the number of unique hscode values\ncat(\"Number of unique hscode values in MC2_edges_unique:\", num_unique_hscode, \"\\n\")\n\n\nNumber of unique hscode values in MC2_edges_unique: 4761 \n\n\nTo further explore this, hscodes were compared with to key sources:\n\nThe Singapore Trade Classification, Customs and Excise Duties document.\nHSN Code List from here.\n\n\n\n\n\n\n\nNote\n\n\n\nSingapore adopts the 8-digit HS Codes in the ASEAN Harmonised Tariff Nomenclature (AHTN), which is based on the WCO 6-digit level HS Codes. We will be using the first 6 digits to match with codes in our dataset.\n\n\nUsing both HS code documents as a guide, the following categories were found to be relevant to our area of interest:\n\n\n\n\n\n\n\n\nNo.\nCategories\nCorresponding Codes\n\n\n\n\n1\nChapter 3: Fish and crustaceans, molluscs and other aquatic invertebrates.\n0301, 302, 303, 304, 305, 306, 307, 308\n\n\n2\nChapter 15: Animal, vegetable or microbial fats and oils and their cleavage products\n1504\n\n\n3\nChapter 16: Preparations of meat, of fish, crustaceans, molluscs or other aquatic 73 invertebrates, or of insects.\n1603, 1604, 1605\n\n\n4\nChapter 21: Flours, meals and pellets, of meat or meat offal, of fish or of crustaceans, molluscs or other aquatic invertebrates, unfit for human consumption; greaves\n2301\n\n\n\nUsing the stringr() and filter() function, we will extract shipments that match the first few digits from the table above.This narrows down the number of relevant links to 752,784.\n\n\nShow the Code\ndesired_hscodes <- c(\"301\", \"302\", \"303\", \"304\", \"305\", \"306\", \"307\", \"308\", \"309\", \"1504\", \"1603\", \"1604\", \"1605\", \"2301\")\n\nMC2_edges_fishing <- MC2_edges_unique %>%\n  filter(str_detect(hscode, paste0(\"^(\", paste(desired_hscodes, collapse = \"|\"), \")\")))\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn this code, the following functions are used:\n\nstr_detect() to check if the “hscode” value starts with any of the codes in the “desired_hscodes” list.\npaste0() function to concatenate the desired codes with the | character to create a regular expression pattern. The pattern ^( and ) ensure that the match occurs at the beginning of the string.\n\n\n\n\n\n3.5 Wrangling attributes\nA close examination of the ’MC2_edges_fishing’ dataframe reveals that it consists of individual transactions. This is not very useful for visualisation.\nIn view of this, we will aggregate the rows by source, target and year month. To do so, we will use the following 4 functions from dplyr package; filter(), group(), summarise(), and ungroup().\nThe new dataframe, edges_ym, will also include of the following variables:\n\nweight: Number of shipments between two entities\nsumweightkg: Total weight of products for all shipments\navg_weight_per_shipment: Average weight of products per shipment (sumweightkg/weight)\nyear: Extracted from yearmonth\n\n\n\n\n\n\n\nNote\n\n\n\nAs the remaining variables such as valueofgoods_omu, volumeteu and valueofgoodsusd recorded a high proportion of ‘NA’ or ‘O’, we will exclude them the new dataframe.\n\n\n\n\nShow the Code\nedges_ym <- MC2_edges_fishing %>%\n  group_by(source, target, yearmonth) %>%\n  summarise(weight = n(), sumweightkg = sum(weightkg), .groups = 'drop') %>%\n  filter(source != target) %>%\n  filter(weight > 1) %>%\n  ungroup() %>%\n  mutate(avg_shipmentwt = sumweightkg / weight) %>%\n  mutate(year = substr(yearmonth, 1, 4))\n\n# Change variable names\nedges_ym <- edges_ym %>%\n  rename(from = source, to = target)\n\n\nLastly, since a signification proportion of edges were filtered out, we will also create a new nodes list.\nIf you recall earlier, more than half of the nodes in the original data set indicated their most associated shipping country as ‘NA’. This number has decreased significantly post-cleaning to about 25%. We will replace ‘NA’ with ‘Unknown’.\n\n\nShow the Code\n# Get unique values from 'source' and 'target' columns\nunique_outliers <- unique(c(edges_ym$from, edges_ym$to))\n\n# Filter 'MC2_nodes' based on unique values\nnodes_ym<- MC2_nodes %>%\n  filter(id %in% unique_outliers)\n\n# Check for NA\nnodes_ym %>%\n  summarise_all(~ sum(is.na(.)) / n())\n\n\n# A tibble: 1 × 3\n     id shpcountry rcvcountry\n  <dbl>      <dbl>      <dbl>\n1     0      0.243      0.158\n\n\nShow the Code\n# Replace with unknown\nnodes_ym <- nodes_ym %>%\n  mutate(\n    shpcountry = if_else(is.na(shpcountry), \"Unknown\", shpcountry),\n    rcvcountry = if_else(is.na(rcvcountry), \"Unknown\", rcvcountry)\n  )"
  },
  {
    "objectID": "Take-Home_Exercises/Take-Home_Ex02/take-home_Ex02_v2.html#understanding-the-data",
    "href": "Take-Home_Exercises/Take-Home_Ex02/take-home_Ex02_v2.html#understanding-the-data",
    "title": "Take Home Exercise 2",
    "section": "4 Understanding the data",
    "text": "4 Understanding the data\n\n4.1 Basic Network Measures\nWith the basic data cleaning completed, we will use tbl_graph() to buid a tidygraph network graph dataframe. Nodes and edges aside, we will also calculate the centrality measures for the network, as it will provide useful insights and guides us to identify important nodes in the network. Specifically, we will calculate:\n\nin-degree - The number of in-degree refers to the number of incoming edges that are directed towards that node. For this exercise, nodes with high in-degree could potentially represent big wholesalers in the fishing industry.\nout-degree - Out-degree of a node refers to the number of outgoing edges that are directed away from that node. In this case, nodes with high out degrees would likely signal big players in the industry.\nbetweenness centrality - This measures of the importance or centrality of a node within a network, as these nodes usually act as a bridge, controlling the flow of goods between nodes.\n\n\n\nShow the Code\nym_graph <- tbl_graph(nodes = nodes_ym,\n                      edges = edges_ym, \n                      directed = TRUE)\n\n# Calculate in-degree and out-degree\nym_graph <- ym_graph %>%\n  mutate(in_degree = degree(., mode = \"in\"),\n         out_degree = degree(., mode = \"out\"))\n\n# Calculate betweenness centrality\nym_graph <- ym_graph %>%\n  mutate(betweenness = centrality_betweenness())\n\n\nA boxplot is then charted to help us better understand the characteristics of the centrality scores that were calculated.\n\n\n\n\n\n\nNote\n\n\n\nFrom the boxplot below, the following observations were noted:\n\nMajority of nodes have very low scores across all 3 centrality measures.\nA sizable group of outliers were noted recording very high betweenness scores. If you recall, betweenness centrality measures the importance of a nodes within a network, as these nodes generally serves are important bridges between other nodes and control the flow.\nIn particular, we see 4 nodes with exceptionally high betweennes scores.\n\n\n\n\n\nShow the Code\n# Pull centrality data\ncentrality_data <- ym_graph %>%\n  select(id, in_degree, out_degree, betweenness) %>%\n  as_tibble() %>%\n  mutate(ID = as.character(id)) %>%\n  select(-id) %>%\n  pivot_longer(-ID, names_to = \"Centrality\", values_to = \"Score\")\n\n# Calculate means\nmeans <- centrality_data %>%\n  group_by(Centrality) %>%\n  summarise(Mean = mean(Score))\n\n# Create tooltip text\ntooltip_text <- centrality_data %>%\n  mutate(Tooltip = paste(\"Centrality:\", Centrality, \"<br>Score:\", Score)) %>%\n  select(Centrality, Tooltip) %>%\n  unique()\n\n# Visualize the range of scores using a boxplot\nplot <- ggplot(centrality_data, aes(x = Centrality, y = Score)) +\n  geom_boxplot(fill = \"steelblue\", color = \"black\") + \n  geom_point(data = means, aes(y = Mean), color = \"red\", shape = 18, size = 3) +\n  labs(x = \"Centrality\", y = \"Score\") +\n  theme_classic() +\n  scale_x_discrete(labels = c(\"Betweeness Centrality\", \"In-Degree\", \"Out-Degree\")) +\n  ggtitle(\"Summary of Centrality Scores\")\n\n# Convert to interactive plot with tooltips\nplotly_plot <- ggplotly(plot, tooltip = \"text\", text = tooltip_text) %>%\n  layout(hoverlabel = list(bgcolor = \"white\", font = list(size = 12)))\n\n# Display the interactive plot\nplotly_plot\n\n\n\n\n\n\nExtracting the company data, we are able to identify the top 4 companies with the highest betweeness scores. Let’s take a closer look at these nodes.\n\n\nShow the Code\n# Sort the centrality_data by betweenness score in descending order\ntop_nodes <- centrality_data %>%\n  filter(Centrality == \"betweenness\") %>%\n  top_n(4, Score) %>%\n  arrange(desc(Score))\n\n# Create a table with two columns: Node ID and Betweenness Score\ntop_nodes_table <- tibble(Node_ID = top_nodes$ID, Betweenness_Score = top_nodes$Score)\n\n# Format the table using kable and kableExtra\nformatted_table <- top_nodes_table %>%\n  kable(col.names = c(\"Node ID\", \"Betweenness Score\"), align = \"c\") %>%\n  kable_styling()\n\nformatted_table\n\n\n\n\n \n  \n    Node ID \n    Betweenness Score \n  \n \n\n  \n    Shou gan  Sagl Mudflat \n    147507.0 \n  \n  \n    Marine Mates NV Worldwide \n    136662.8 \n  \n  \n    Playa de la Felicidad Ltd Consultants \n    134826.5 \n  \n  \n    Adriatic Tuna Seabass BV Transit \n    119693.0 \n  \n\n\n\n\n\nTo narrow down dataset, the code chunk below helps to extract relevent nodes and edges in two formats:\n\nNodes and edges associated with the top company, i.e. Shou gan Sagl Mudflat\nNodes and edges associated with the top 4 companies\n\n\n\n\n\n\n\nNote\n\n\n\nAligned with earlier approach, we will aggregate both edges by source, target and year month using the group_by(), summarise() and filter() functions.\n\n\n\n\nShow the Code\n###Top node\n# Filter rows based on company names in 'from' or 'to' column for Shou gan Sagl Mudflat\ntopedge_ym <- MC2_edges_fishing %>%\n  filter(source == \"Shou gan  Sagl Mudflat\" | target == \"Shou gan  Sagl Mudflat\")\n\n#Aggregate data\ntopedge_ym_agg <- topedge_ym %>%\n  group_by(source, target, yearmonth) %>%\n  summarise(weight = n(), sumweightkg = sum(weightkg), .groups = 'drop') %>%\n  filter(source != target) %>%\n  filter(weight > 1) %>%\n  ungroup() %>%\n  mutate(avg_shipmentwt = sumweightkg / weight) %>%\n  mutate(year = substr(yearmonth, 1, 4))\n\n# Change variable names\ntopedge_ym_agg <- topedge_ym_agg %>%\n  rename(from = source, to = target)\n\n# Get unique values from 'source' and 'target' columns\nunique_outliers <- unique(c(topedge_ym_agg$from, topedge_ym_agg$to))\n\n# Filter 'MC2_nodes' based on unique values\ntopenode_ym_agg<- MC2_nodes_unique %>%\n  filter(id %in% unique_outliers)\n\n\n###Top 4 nodes\n# Define the four company names\ncompanies <- c(\"Shou gan  Sagl Mudflat\", \"Marine Mates NV Worldwide\", \"Playa de la Felicidad Ltd Consultants\", \"Adriatic Tuna Seabass BV Transit\")\n\n# Filter rows based on company names in 'from' or 'to' column\ntop4_edge_ym <- MC2_edges_fishing %>%\n  filter( source %in% companies | target %in% companies)\n\n#Aggregate data\ntop4_edge_ym_agg <- top4_edge_ym %>%\n  group_by(source, target, yearmonth) %>%\n  summarise(weight = n(), sumweightkg = sum(weightkg), .groups = 'drop') %>%\n  filter(source != target) %>%\n  filter(weight > 1) %>%\n  ungroup() %>%\n  mutate(avg_shipmentwt = sumweightkg / weight) %>%\n  mutate(year = substr(yearmonth, 1, 4))\n\n# Change variable names\ntop4_edge_ym_agg <- top4_edge_ym_agg %>%\n  rename(from = source, to = target)\n\n# Get unique values from 'source' and 'target' columns\nunique_outliers <- unique(c(top4_edge_ym_agg$from, top4_edge_ym_agg$to))\n\n# Filter 'MC2_nodes' based on unique values\ntop4_nodes_ym_agg<- MC2_nodes_unique %>%\n  filter(id %in% unique_outliers)\n\n\n\n\n4.3 Business Patterns of Node with Highest Betweenness - Shou gan Sagl Mudflat\nUsing a heat map, we can plot out the shipment patterns of ‘Shou gan Sagl Mudflat’ across the 6 years of data. The following trends were observed:\n\nThe company started out their business mostly shipping produce between 2028 to 2030. During this period, while they do receive shipments, it is only with a few trading partners e.g. Danish Plaice Swordfish AB Shipping\nFrom about the mid 2030s, the company starting receiving shipments from an increased number of companies. The business relationship were a mix of sporadic and constant. It is likely during this period that the company started to record high betweenness score.\n\n\n\n\n\n\n\nNote\n\n\n\nIt is curious to note quite a handful of transactions across several trading partners being fairly sporadic, or even one-off. To understand if this is a norm within the industry, we will review the trading patterns for the node with the next highest betweenness score; Marine Mates NV Worldwide, for comparison.\n\n\n\n\nShow the Code\n# Define custom color palette from blue to yellow\nmy_palette <- c(\"#0000FF\", \"#FFFF00\", \"#FF0000\")  # Example colors\n\n# Custom formatting function for legend labels\nformat_labels <- function(x) {\n  format(round(x), nsmall = 0)\n}\n\n# Plot heatmap with custom color palette\nggplot(topedge_ym_agg, aes(x = yearmonth, y = from, fill = weight)) +\n  geom_tile() +\n  labs(x = \"Time Period\", y = \"Source\", fill = \"Weight\") +\n  scale_fill_gradientn(colors = my_palette, limits = range(topedge_ym_agg$weight),\n                       breaks = seq(min(topedge_ym_agg$weight), max(topedge_ym_agg$weight), length.out = length(my_palette)),\n                       labels = format_labels,\n                       guide = guide_colorbar(title.position = \"top\")) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 0, hjust = 1),\n        axis.text.y = element_text(size = 3),\n        plot.margin = margin(20, 20, 20, 120),\n        legend.position = \"bottom\",\n        legend.key.height = unit(0.3, \"cm\"),\n        legend.key.width = unit(1.5,\"cm\"),\n        plot.title = element_text(face = \"bold\")) +\n  ggtitle(\"Shipment Patterns across 6 years\")\n\n\n\n\n\n\n\n4.3 Business Patterns of Node with Highest Betweenness - Marine Mates NV Worldwide\nSince we did not create a data frame for Marine Mates NV Worldwide earlier, the following code chunk will do so.\n\n\nShow the Code\n# Filter rows based on company names in 'from' or 'to' column for Marine Mates NV Worldwide\nmarine_edge_ym <- MC2_edges_fishing %>%\n  filter(source == \"Marine Mates NV Worldwide\" | target == \"Marine Mates NV Worldwide\")\n\n#Aggregate data\nmarine_edge_ym_agg <- marine_edge_ym %>%\n  group_by(source, target, yearmonth) %>%\n  summarise(weight = n(), sumweightkg = sum(weightkg), .groups = 'drop') %>%\n  filter(source != target) %>%\n  filter(weight > 1) %>%\n  ungroup() %>%\n  mutate(avg_shipmentwt = sumweightkg / weight) %>%\n  mutate(year = substr(yearmonth, 1, 4))\n\n# Change variable names\nmarine_edge_ym_agg <- marine_edge_ym_agg %>%\n  rename(from = source, to = target)\n\n# Get unique values from 'source' and 'target' columns\nunique_outliers <- unique(c(marine_edge_ym_agg$from, marine_edge_ym_agg$to))\n\n# Filter 'MC2_nodes' based on unique values\nmarine_node_ym_agg<- MC2_nodes_unique %>%\n  filter(id %in% unique_outliers)\n\n\n\n\nShow the Code\n# Define custom color palette from blue to yellow\nmy_palette <- c(\"#0000FF\", \"#FFFF00\", \"#FF0000\")  # Example colors\n\n# Custom formatting function for legend labels\nformat_labels <- function(x) {\n  format(round(x), nsmall = 0)\n}\n\n# Plot heatmap with custom color palette\nggplot(marine_edge_ym_agg, aes(x = yearmonth, y = from, fill = weight)) +\n  geom_tile() +\n  labs(x = \"Time Period\", y = \"Source\", fill = \"Weight\") +\n  scale_fill_gradientn(colors = my_palette, limits = range(marine_edge_ym_agg$weight),\n                       breaks = seq(min(marine_edge_ym_agg$weight), max(marine_edge_ym_agg$weight), length.out = length(my_palette)),\n                       labels = format_labels,\n                       guide = guide_colorbar(title.position = \"top\")) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 0, hjust = 1),\n        axis.text.y = element_text(size = 3),\n        plot.margin = margin(20, 20, 20, 120),\n        legend.position = \"bottom\",\n        legend.key.height = unit(0.3, \"cm\"),\n        legend.key.width = unit(1.5,\"cm\"),\n        plot.title = element_text(face = \"bold\")) +\n  ggtitle(\"Shipment Patterns across 6 years\")\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLooking at the heatmap for Marine Mates NV Worldwide, we see that trading patterns are even more sparse compared to Shou gan Sagl Mudflat. This suggests that perhaps, such trading patterns are the norm within the industry.\nWhile it is interesting to review the trading patterns of the individual nodes, insights are fairly limited to help us better understand whether a company is involved in illegal activities. As such, we will expand out lens in subsequent analyses and look at the top 4 nodes.\n\n\n\n\n4.4 Network Graph - Top 4 Betweenness Nodes\nAs hypothesized, the network graph with the top 4 nodes indeed looks more interesting. Specifically, we can see 4 prominent clusters of nodes. But more interestingly, we also see a handful of nodes that don’t seems to fit squarely into any specific cluster, but yet at the same time hold strong relationships (represented by the darker edges).\n\n\n\n\n\n\nNote\n\n\n\nWhile it is likely too prematurely to make a call, but these ‘in-between’ nodes may be useful data points to study. Perhaps, they could represent illegal fishing companies that trade between several large communities.\nTo study this, we will look at the in and out degree of these nodes.\n\n\n\n\nShow the Code\nvisNetwork(\n  nodes = top4_nodes_ym_agg,\n  edges = top4_edge_ym_agg,\n  main = \"Network Graph of Top 4 Betweenness Nodes\"\n) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visOptions(\n    highlightNearest = TRUE,\n    nodesIdSelection = TRUE\n  ) %>%\n  visEdges(\n    arrows = \"to\",\n    smooth = list(enabled = TRUE, type = \"curvedCW\"),\n    color = list(\n      inherit = FALSE,\n      color = top4_edge_ym_agg$weight,\n      opacity = 0.2\n    )\n  ) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n4.5 Network Graph - In and Out Degree\nIf the nodes floating between the 4 large clusters were indeed provider of goods, they will likely register high out degree, and low in degree i.e., they should not be purchasers of produce. However, looking the network graphs below, these nodes tended to have higher in degrees versus out degrees.\nThis suggests that there are businesses who purchases products, likely across clusters. And unfortunately, these nodes seems unlikely to be contributing to illegal fishing.\n\n\nShow the Code\nset.seed(1234)\n\ntop4_graph <- tbl_graph(nodes = top4_nodes_ym_agg,\n                           edges = top4_edge_ym_agg, \n                           directed = TRUE)\n\n# Calculate in-degree values\ntop4_graph <- top4_graph %>%\n  activate(nodes) %>%\n  mutate(in_degree = centrality_degree(mode = \"in\"))\n\n# Plot the graph by in degree\ng <- ggraph(top4_graph, layout = 'fr') +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_edge_link(aes(width = weight), color = \"grey\", alpha = 0.2) +\n  geom_node_point(aes(size = in_degree), color = \"lightblue\", alpha = 0.6) +\n  scale_size(range = c(1, 10)) +\n  theme_graph() +\n  ggtitle(\"Top 4 nodes - In-degree\")\n\ng\n\n\n\n\n\n\n\nShow the Code\nset.seed(1234)\n\n# Calculate out-degree values\ntop4_graph <- top4_graph %>%\n  activate(nodes) %>%\n  mutate(in_degree = centrality_degree(mode = \"out\"))\n\n# Plot the graph by in degree\ng <- ggraph(top4_graph, layout = 'fr') +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_edge_link(aes(width = weight), color = \"grey\", alpha = 0.2) +\n  geom_node_point(aes(size = in_degree), color = \"orange\", alpha = 0.6) +\n  scale_size(range = c(1, 10)) +\n  theme_graph() +\n  ggtitle(\"Top 4 nodes - Out-degree\")\n\ng\n\n\n\n\n\n\n\n4.4 Network Graph - Network Community\nIn order to detect communities in our network, the igraph library is used. As we has a directed graph, cluster_walktrap() function is used to perform community detection using the Walktrap algorithm.\nSince our data focuses on 4 key nodes, it was unsurprising that 4 clusters appeared. It was also noted that the ‘in-between’ nodes were also clustered accordingly. Nodes with higher betweenness tended to ‘absorb’ more of these ‘in-between’ nodes.\n\n\nShow the Code\n# Convert tbl_graph to igraph object\ntop4_igraph <- as.igraph(top4_graph)\n\n# Perform community detection using Walktrap\ntop4_communities <- cluster_walktrap(top4_igraph)\n\n# Add community information to the graph nodes\ntop4_graph <- top4_graph %>%\n  activate(nodes) %>%\n  mutate(community = membership(top4_communities))\n\n# Plot the graph with community colors\ng <- ggraph(top4_graph, layout = 'fr') +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_edge_link(aes(width = weight), color = \"grey\", alpha = 0.2) +\n  geom_node_point(aes(size = 0.2, color = as.factor(community)), alpha = 0.6) +\n  scale_size(range = c(1, 3)) +\n  scale_color_discrete(guide = FALSE) +\n  theme_graph() +\n  ggtitle(\"Top 4 nodes - Community Detection\")\n\ng"
  },
  {
    "objectID": "Take-Home_Exercises/Take-Home_Ex02/take-home_Ex02_v2.html#conclusion",
    "href": "Take-Home_Exercises/Take-Home_Ex02/take-home_Ex02_v2.html#conclusion",
    "title": "Take Home Exercise 2",
    "section": "5 Conclusion",
    "text": "5 Conclusion\n\nLooking at the overall dataset provided, a sizable proportion of businesses registered very low betweenness, in and out degrees. This suggests that the industry is likely fairly fragmented, making the identification of illegal activities all the more challenging.\nFor the purpose of these exercises, the top 4 nodes with the highest betweenness scores were identified. These scores for these nodes were significantly higher when compared to the average nodes. This suggests that they are likely big players, be it a supplier or purchasers of produce from the industry.\nZooming int the top 2 players, it was observed that trading patterns between partners were a mix of stable and sporadic. In fact, a large majority of trading relations were sporadic, some even one-off.\nLooking at the network graph of the top 4 players, we noticed there were a handful of smaller businesses that interact with multiple large nodes. When looking at the in and out-degree of these ‘in-between’ nodes, these nodes tended to have higher in degrees versus out degree, suggesting that there are businesses who purchases instead of supply produce. As such, these nodes seem unlikely to be contributing to illegal fishing activites."
  },
  {
    "objectID": "Take-Home_Exercises/Take-Home_Ex02/take-home_Ex02_v2.html#suggestion-for-group-project",
    "href": "Take-Home_Exercises/Take-Home_Ex02/take-home_Ex02_v2.html#suggestion-for-group-project",
    "title": "Take Home Exercise 2",
    "section": "6 Suggestion for Group Project",
    "text": "6 Suggestion for Group Project\n\nAs part of this exercise, I tried plotting the larger network of the 4 key nodes, i.e., beyond the 4 ego-network, however it crashed my computer. That said, I believe there is value in exploring the peripheral nodes, perhaps be more selective, and identify nodes that are more isolated than others. Specifically, we will look out for nodes that constantly make one-off trade, or tend to have large intervals between them.\nCurrently, this exercise only look at number of shipments and its associated pattern. However, they is also value in looking at other measures such as total weight, or average weight per shipment, to understand if there are odd relationships on that front."
  }
]